{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74e28739",
   "metadata": {},
   "source": [
    "This notebook is for development! Use it to try out new strategies :) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6284fcd3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Imports:</b> Common imports for NumPy, MatplotLib, Pandas, and OS </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04c47bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import for data generation\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e4f250",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>ML Imports:</b> Imports to execute RNN model for sequential data </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b788d831",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "import tensorflow as tf\n",
    "from pyspark import SparkContext\n",
    "from sklearn import decomposition\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de736508",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>nn_train_test() function:</b> \n",
    "    \n",
    "This function takes inputs:\n",
    "    \n",
    "    path --> path to the repository with all your .npy file\n",
    "    n_future --> number of future days of fire you want to predict (will be 1 for us)\n",
    "    n_comp --> number of principal componenents you want PCA to reduce to\n",
    "    n_past --> number of past days you want to consider (we can experiment with this)\n",
    "    \n",
    "Outputs:\n",
    "    \n",
    "    input_dim is set to a 1-D array of 256 ^ 2 for now! \n",
    "    x_train --> x_train in format (batch_size, timesteps, input_dim)\n",
    "    y_train --> training data in format (batch_size, input_dim)\n",
    "    pca_array --> an array of all of the PCA's done to the x_training data in a row. This is useful when computing the \n",
    "    inverse of the results of your neural net!\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d6ead9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>nn_train_test() function:</b> \n",
    "    \n",
    "This function creates the train and test sets a little differently. Instead of using pca to reduce dimensionality, it keeps all the data in anticipation of the Neural Net Framework to deal with filtering through it. The d variable allows you to discretize the grid down further as well.\n",
    "    \n",
    "    path --> path to the repository with all your .npy file\n",
    "    n_future --> number of future days of fire you want to predict (will be 1 for us)\n",
    "    n_comp --> Doesn't do anything right now\n",
    "    n_past --> number of past days you want to consider (we can experiment with this)\n",
    "    d --> The size of the discretized fires you want (i.e. d= 32, returns subset of sequences for 32x32 grids)\n",
    "    \n",
    "Outputs:\n",
    "    \n",
    "    input_dim is set to a 1-D array of 256 ^ 2 for now! \n",
    "    x_train --> x_train in format (batch_size, timesteps, input_dim)\n",
    "    y_train --> training data in format (batch_size, input_dim)\n",
    "    pca_array -->  Nothing\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "e639e5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_train_test(path, n_past, n_comp, n_future = 1, d = 256):\n",
    "    x_train_list = []\n",
    "    y_train_list = []\n",
    "    count = 0\n",
    "    pca_array = []\n",
    "    for file in os.listdir(r\"{path}\".format(path = path)):\n",
    "        if os.path.getsize(\"{path}\\\\{file}\".format(path = path, file = file)) < 10000000:\n",
    "            continue\n",
    "        with open(\"{path}\\\\{file}\".format(path = path, file = file), 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        training_fire = data['multiDay']\n",
    "        elevation_data = data['Elevation Data'][:-1,:-1]\n",
    "        elev_shape = np.array(elevation_data.shape)\n",
    "        elevation_fire = np.zeros((256, 256))\n",
    "        shape = np.array(training_fire.shape) \n",
    "        fire = np.where(training_fire[1] == 1)\n",
    "        x_center, y_center = int(np.median(fire[0])), int(np.median(fire[1]))\n",
    "        if shape[0] < n_past + n_future:\n",
    "            continue\n",
    "        standard_fire = np.zeros((len(training_fire), 256, 256))    \n",
    "        if shape[1] > 255 or shape[2] > 255:\n",
    "\n",
    "\n",
    "            # could not broadcast input array from shape (39,350,325) into shape (39,512,416)\n",
    "            xLow = x_center - 128 \n",
    "            xHi = shape[1] - x_center\n",
    "            yLow = y_center - 128\n",
    "            yHi = shape[2] - y_center\n",
    "            print(xLow, xHi, yLow, yHi)\n",
    "            if shape[1] > 255:\n",
    "                if xLow < 0:\n",
    "                    xHi = x_center + 128 - xLow\n",
    "                    xLow = 0\n",
    "                elif xHi < 128:\n",
    "                    xLow = x_center - 256 + xHi\n",
    "                    xHi = shape[1]\n",
    "                else:\n",
    "                    xLow = x_center - 128\n",
    "                    xHi = x_center + 128\n",
    "            else:\n",
    "                xLow = 0\n",
    "                xHi = shape[1]\n",
    "\n",
    "            if shape[2] > 255:\n",
    "                if yLow < 0:\n",
    "                    yHi = y_center + 128 - yLow\n",
    "                    yLow = 0\n",
    "                elif yHi < 128:\n",
    "                    yLow = y_center - 256 + yHi\n",
    "                    yHi = shape[2]\n",
    "                else:\n",
    "                    yLow = y_center - 128\n",
    "                    yHi = y_center + 128\n",
    "            else:\n",
    "                yLow = 0\n",
    "                yHi = shape[2]\n",
    "            print(xLow, xHi, yLow, yHi)\n",
    "            standard_fire[:shape[0], :min(256, shape[1]), :min(256, shape[2])] = training_fire[:, xLow: xHi, yLow: yHi]   \n",
    "            elevation_fire[:min(256, elev_shape[0]), :min(256, elev_shape[1])] = elevation_data[xLow: xHi, yLow: yHi]\n",
    "        else: \n",
    "            standard_fire[:shape[0], :shape[1], :shape[2]] = training_fire\n",
    "            elevation_fire[:min(256, elev_shape[0]), :min(256, elev_shape[1])] = elevation_data\n",
    "\n",
    "        pca_fire = standard_fire\n",
    "        pca_elev = elevation_fire.reshape((1, 256, 256))\n",
    "        for i in range(0 , len(training_fire) - n_future - n_past + 1):\n",
    "            shape = pca_fire[i : i + n_past].shape\n",
    "            a = np.zeros((shape[0] + 1, shape[1], shape[2]))\n",
    "            a[:3] = pca_fire[i : i + n_past]\n",
    "            a[3:] = pca_elev \n",
    "            for j in np.arange(0, 255, d):\n",
    "                for k in np.arange(0, 255, d):\n",
    "                    x_train_list.append(a[:, j:j+d, k:k+d])\n",
    "                    y_train_list.append(pca_fire[i + n_past: i + n_past + n_future, j:j+d, k:k+d])\n",
    "    \n",
    "    x_train = np.array(x_train_list)\n",
    "    y_train = np.array(y_train_list)  \n",
    "    \n",
    "    #x_train = np.reshape(x_train, (x_train.shape[0],  x_train.shape[2], x_train.shape[1]))\n",
    "    #y_train = np.reshape(y_train, (y_train.shape[0], y_train.shape[2]))    \n",
    "    print(x_train.shape, y_train.shape, len(pca_array))\n",
    "    return x_train, y_train, pca_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098aad49",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>RNN Inputs:</b> \n",
    "We are declaring below that the number of future days we want to predict is 1, and we can vary the number of past days\n",
    "    we want to predict (I have the default as 3 for now)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "176016f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_future = 1\n",
    "n_comp = 10\n",
    "n_past = 3\n",
    "discretization_amount = d = 256 # 256 means we keep the whole fire grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80833767",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Input Data for Training:</b> \n",
    "    Below, we get the input data from December 2018! \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "78caa976",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture \n",
    "# We are suppressing print statements and warning messages w/ above line\n",
    "path = \"C:\\\\Users\\\\nico\\\\Desktop\\\\Stanford\\\\OneDrive - Stanford\\\\Courses\\\\CS229\\\\finalproject\\\\data\\\\United_States_Fires\\\\United_States_2018_Fires\\\\dec\\\\storage\"  \n",
    "x_train, y_train, pca_array  = nn_train_test(path, n_past = n_past, n_comp = n_comp, n_future = n_future, d = d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "a84dfb53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((408, 4, 256, 256), (408, 1, 256, 256))"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd83f0c",
   "metadata": {},
   "source": [
    "Adding 0 to 3 \\\n",
    "3 \\\n",
    "Adding 1 to 4 \\\n",
    "4 \\\n",
    "Adding 2 to 5 \\\n",
    "5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5265e3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Check:</b> \n",
    "    Check the shape of your x_train and y_train data\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "30ed7348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((26112, 4, 32, 32), (26112, 1, 32, 32))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61afbcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "5cd966f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 256, 256, 4)"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "5de87020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<matplotlib.image.AxesImage at 0x242d02e74f0>,\n",
       " <matplotlib.image.AxesImage at 0x242d0321910>)"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQwAAAECCAYAAAALhunjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAOLUlEQVR4nO3dX6hdZ5nH8e9vaoyoFZvpTEjTMK2SuagXE0uoBYt0EKzNTfSmtBeakUK8aEHBAaNe2BvBGcYKwkwh0mId1E5BpWHojNagyFxYTUtMk3Y6jVppYprMjIMWZWJbn7k4q3SbOSfnOTn77D853w9s9trvWmuf57w5+fGu9a61d6oKSer4o2kXIGl+GBiS2gwMSW0GhqQ2A0NSm4EhqW3qgZHkvUmeTnI8yb5p17OYJM8meSLJ4SSHhrZNSR5J8szwfNkU67svyZkkR0faFq0vC74w9PeRJNfOQK13JTk59O/hJLtG1n1iqPXpJDdNuNZtSb6b5Mkkx5J8ZGif1b5dqt7x9W9VTe0BXAL8BHgL8Frgx8A106xpiTqfBS4/p+1vgX3D8j7gb6ZY37uAa4Gjy9UH7AL+BQhwPfDoDNR6F/DXi2x7zfA3sRG4evhbuWSCtW4Brh2WLwX+Y6hpVvt2qXrH1r/THmFcBxyvqp9W1e+AB4DdU66pazdw/7B8P/C+aRVSVd8HfnlO81L17Qa+XAt+ALw5yZaJFMqStS5lN/BAVZ2tqp8Bx1n4m5mIqjpVVY8Pyy8ATwFbmd2+Xarepay4f6cdGFuB50Zen+D8v+C0FPDtJI8l2Tu0ba6qU8Py88Dm6ZS2pKXqm9U+v3MYxt83cng3M7UmuQp4O/Aoc9C359QLY+rfaQfGvLihqq4FbgbuSPKu0ZW1ML6b2WvsZ70+4B7grcAO4BTwualWc44kbwS+Dny0qn49um4W+3aResfWv9MOjJPAtpHXVw5tM6WqTg7PZ4BvsjBsO/3KcHN4PjO9Che1VH0z1+dVdbqqXq6q3wNf5NVh8dRrTbKBhf98X6mqbwzNM9u3i9U7zv6ddmD8CNie5OokrwVuBQ5MuaY/kOQNSS59ZRl4D3CUhTr3DJvtAR6aToVLWqq+A8AHhzP61wO/GhleT8U5x/nvZ6F/YaHWW5NsTHI1sB344QTrCnAv8FRV3T2yaib7dql6x9q/kzyLu8SZ3V0snM39CfCpadezSH1vYeFM8o+BY6/UCPwxcBB4BvgOsGmKNX6NhaHmiywch96+VH0snMH/+6G/nwB2zkCt/zjUcmT4I94ysv2nhlqfBm6ecK03sHC4cQQ4PDx2zXDfLlXv2Po3w06StKxpH5JImiMGhqQ2A0NSm4Ehqc3AkNS2ZoGx0rtQRy65nnnzVCvMV73zVCusv3rXJDCSXMLCfPTNLNwRd1uSa5bZbZ46fp5qhfmqd55qhXVW71qNMOb5LlRJS3jNGr3vYnfBvWOpjV+bjfU6Xs+bsmkuriKbp1phvuqdp1ph/uv9X37D7+psuvuvVWAsaziW2gsLv8QNr34IkKQJebQOrmj7tTokWfYuuKraX1U7q2rnBjauURmSxmmtAmPm70KVtHJrckhSVS8luRP4Fguf23lfVR1bi58laXLW7BxGVT0MPLxW7y9p8rzSU1KbgSGpzcCQ1GZgSGozMCS1GRiS2gwMSW0GhqQ2A0NSm4Ehqc3AkNRmYEhqMzAktRkYktoMDEltBoakNgNDUpuBIanNwJDUZmBIajMwJLUZGJLaDAxJbQaGpDYDQ1KbgSGpzcCQ1GZgSGozMCS1GRiS2gwMSW0GhqQ2A0NSm4Ehqe01q9k5ybPAC8DLwEtVtTPJJuCfgKuAZ4Fbqup/VlempFkwjhHGX1bVjqraObzeBxysqu3AweG1pIvAWhyS7AbuH5bvB963Bj9D0hSsNjAK+HaSx5LsHdo2V9WpYfl5YPMqf4akGbGqcxjADVV1MsmfAo8k+ffRlVVVSWqxHYeA2QvwOl6/yjIkTcKqRhhVdXJ4PgN8E7gOOJ1kC8DwfGaJffdX1c6q2rmBjaspQ9KEXHBgJHlDkktfWQbeAxwFDgB7hs32AA+ttkhJs2E1hySbgW8meeV9vlpV/5rkR8CDSW4Hfg7csvoyJc2CCw6Mqvop8BeLtP838O7VFCVpNnmlp6Q2A0NSm4Ehqc3AkNRmYEhqMzAktRkYktoMDEltBoakNgNDUpuBIanNwJDUZmBIalvtJ25JM+9bvzh83vU3XbFjInVcDBxhaN1bLlD0KgNDUpuBIanNwJDUZmBIajMwJLUZGJLaDAxJbQaGpDYDQ1KbgSGpzcCQ1GZgSGozMCS1GRiS2gwMSW0GhqQ2A0NSm4Ehqc3AkNRmYEhqWzYwktyX5EySoyNtm5I8kuSZ4fmyoT1JvpDkeJIjSa5dy+IlTVZnhPEl4L3ntO0DDlbVduDg8BrgZmD78NgL3DOeMiXNgmW/l6Sqvp/kqnOadwM3Dsv3A98DPj60f7mqCvhBkjcn2VJVp8ZWsbRCfu/I+FzoOYzNIyHwPLB5WN4KPDey3YmhTdJFYNUnPYfRRK10vyR7kxxKcuhFzq62DEkTcKGBcTrJFoDh+czQfhLYNrLdlUPb/1NV+6tqZ1Xt3MDGCyxD0iRdaGAcAPYMy3uAh0baPzjMllwP/MrzF9LFY9mTnkm+xsIJzsuTnAA+DXwWeDDJ7cDPgVuGzR8GdgHHgd8CH1qDmiVNSWeW5LYlVr17kW0LuGO1RUmaTV7pKalt2RGGNAu+9YvD513vtRaT4QhDUpuBIanNwJDUZmBIajMwJLUZGJLanFbVzFtuSnW5bZxyHR8DQxe9TuAYKj0ekkhqMzAktRkYktoMDEltBoakNgNDUpvTqpp5N12xozU1uhpex9FjYGgunO8/7STCxNBY4CGJpDYDQ1KbgSGpzcCQ1GZgSGozMCS1Oa2qlln+mP/lfvZaT7uuJwaGxmKtr1WY5cBaTzwkkdRmYEhqMzAktRkYktoMDEltBoakNgNDM8/rKGaHgSGpbdnASHJfkjNJjo603ZXkZJLDw2PXyLpPJDme5OkkN61V4ZImrzPC+BLw3kXaP19VO4bHwwBJrgFuBd427PMPSS4ZV7GSpmvZwKiq7wO/bL7fbuCBqjpbVT8DjgPXraI+STNkNecw7kxyZDhkuWxo2wo8N7LNiaFN0kXgQgPjHuCtwA7gFPC5lb5Bkr1JDiU59CJnL7AMSZN0QYFRVaer6uWq+j3wRV497DgJbBvZ9MqhbbH32F9VO6tq5wY2XkgZkibsgm5vT7Klqk4NL98PvDKDcgD4apK7gSuA7cAPV12ltAqdW9+91qNn2cBI8jXgRuDyJCeATwM3JtkBFPAs8GGAqjqW5EHgSeAl4I6qenlNKpfGyM/T6Fk2MKrqtkWa7z3P9p8BPrOaoiTNJq/0lNRmYEhqMzAktRkYktoMDEltfs3AOrHaj+m/6YodXqsgA0MLOt8rcjFfq+D3nvR4SCKpzcCQ1GZgSGozMCS1GRiS2gwMSW1Oqwq4uKcNvX5kfBxhSGozMCS1GRiS2gwMSW0GhqQ2A0NSm9Oq68Q8T5t6a/3sMDA0F9Y68OY5UCfJQxJJbQaGpDYDQ1KbgSGpzcCQ1GZgSGozMCS1GRiS2gwMSW0GhqQ2A0NSm4Ehqc3AkNS2bGAk2Zbku0meTHIsyUeG9k1JHknyzPB82dCeJF9IcjzJkSTXrvUvIWkyOre3vwR8rKoeT3Ip8FiSR4C/Ag5W1WeT7AP2AR8Hbga2D493APcMz+ue3xA+HX6exvgsGxhVdQo4NSy/kOQpYCuwG7hx2Ox+4HssBMZu4MtVVcAPkrw5yZbhfaSpWC6MDfOeFZ3DSHIV8HbgUWDzSAg8D2welrcCz43sdmJokzTn2oGR5I3A14GPVtWvR9cNo4layQ9OsjfJoSSHXuTsSnaVNCWtwEiygYWw+EpVfWNoPp1ky7B+C3BmaD8JbBvZ/cqh7Q9U1f6q2llVOzew8ULrlzRBnVmSAPcCT1XV3SOrDgB7huU9wEMj7R8cZkuuB37l+Qvp4tCZJXkn8AHgiSSHh7ZPAp8FHkxyO/Bz4JZh3cPALuA48FvgQ+MsWNL0dGZJ/g3IEqvfvcj2BdyxyrokzSCv9NS65zUafX4vidYFQ2E8HGFIajMwJLUZGJLaDAxJbQaGpDYDQ1Kb06oT5C3S0+GU6vgYGGPm5yrMJ/9dejwkkdRmYEhqMzAktRkYktoMDEltBoakNqdVx6gz33++bVYzted0ribBEcY64cVLGgcDQ1KbgSGpzcCQ1GZgSGozMCS1GRiS2gwMSW0GxjrhhVsaBwNDUpuBIanNwJDUZmBIajMwJLUZGJLa/DyMMVpu6nItbzG/mKdNO/12Mf/+s8QRhqQ2A0NS27KBkWRbku8meTLJsSQfGdrvSnIyyeHhsWtkn08kOZ7k6SQ3reUvIGlyOucwXgI+VlWPJ7kUeCzJI8O6z1fV341unOQa4FbgbcAVwHeS/HlVvTzOwiVN3rIjjKo6VVWPD8svAE8BW8+zy27ggao6W1U/A44D142jWEnTtaJzGEmuAt4OPDo03ZnkSJL7klw2tG0FnhvZ7QTnDxhJc6IdGEneCHwd+GhV/Rq4B3grsAM4BXxuJT84yd4kh5IcepGzK9l1bjn1Nx32+/i0rsNIsoGFsPhKVX0DoKpOj6z/IvDPw8uTwLaR3a8c2v5AVe0H9gO8KZvqQoqfR/7xjl+nT+338ejMkgS4F3iqqu4ead8ystn7gaPD8gHg1iQbk1wNbAd+OL6SJU1LZ4TxTuADwBNJDg9tnwRuS7IDKOBZ4MMAVXUsyYPAkyzMsNzhDIl0cUjV9I8Gkvwn8Bvgv6ZdS9PlzE+tMF/1zlOtMP/1/llV/Ul355kIDIAkh6pq57Tr6JinWmG+6p2nWmH91eul4ZLaDAxJbbMUGPunXcAKzFOtMF/1zlOtsM7qnZlzGJJm3yyNMCTNOANDUpuBIanNwJDUZmBIavs/Oj3yuPA8NjwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#k = np.random.choice(np.where(y_train > 0)[0])\n",
    "b = x_train[k].reshape((4, 256, 256))\n",
    "plt.matshow(b[2]), plt.matshow(y_train[k].reshape((256, 256)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "c0969824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((408, 4, 1, 256, 256), (408, 1, 65536))"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8fc63d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    This follows the model in https://towardsdatascience.com/wildfire-spreading-modeling-in-alberta-canada-a-trial-using-a-neural-network-with-convlstm-cells-81c1a9f7d410. Their GitHub is at the bottom of the page \n",
    "    \n",
    "<b>Train your RNN:</b> Train your RNN using the training data.\n",
    "    \n",
    "    You might notice a lot of arbitrary values here (these are things we will want to change and test)\n",
    "    - Dropout: High dropout leads to more generalization. Low dropout takes advantage of more data but overfits more easily\n",
    "    - # of hidden layers: Right now, there are 2 hidden layers (+ 1 at the end). We can change this\n",
    "    - # units at hidden layers: Right now, units decrease by 1 at each hidden layer. This is arbitrary and can be changed\n",
    "    - optimizer --> 'adam' works but I don't know what it does\n",
    "    - epochs --> We can raise this above 1 but I don't notice that changing much when I have done so\n",
    "    - batch_size --> Higher batch size leads to more generalization\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef239d9",
   "metadata": {},
   "source": [
    "Reshape your data as needed for the ConvLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "0195febd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = x_train.reshape((int(408 * 256**2/d**2), 4, d, d, 1)), y_train.reshape((int(408 * 256**2/d**2), d, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b19fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ba0d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "223/408 [===============>..............] - ETA: 9:56 - loss: 0.0388 - acc: 0.0000e+00 "
     ]
    }
   ],
   "source": [
    "# No need to run this if you are loading a model\n",
    "regressor = Sequential()\n",
    "\n",
    "dropout = 0.1\n",
    "\n",
    "#regressor.add(tf.keras.layers.Masking(mask_value=-9999,input_shape= x_train[0].shape)) \n",
    "regressor.add(tf.keras.layers.ConvLSTM2D(filters = 64, kernel_size = (3,3), return_sequences = True, data_format='channels_first', activation = \"tanh\", input_shape = x_train[0].shape))\n",
    "\n",
    "regressor.add(tf.keras.layers.MaxPool3D(pool_size = (2,2,2)))\n",
    "\n",
    "#regressor.add(tf.keras.layers.ConvLSTM2D(filters = 64, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
    "\n",
    "#regressor.add(tf.keras.layers.MaxPool2D(pool_size = (2,2)))\n",
    "\n",
    "regressor.add(tf.keras.layers.Flatten())\n",
    "\n",
    "regressor.add(tf.keras.layers.Dense(units = 128, activation = 'relu'))\n",
    "\n",
    "regressor.add(tf.keras.layers.Dropout(0.1))\n",
    "\n",
    "regressor.add(tf.keras.layers.Dense(units = 256, activation = 'relu'))\n",
    "\n",
    "regressor.add(tf.keras.layers.Dropout(0.1))\n",
    "\n",
    "\n",
    "regressor.add(tf.keras.layers.Dense(units = 256*256, activation = 'relu'))\n",
    "\n",
    "'''\n",
    "regressor.add(tf.keras.layers.Bidirectional(LSTM(units=n_past, return_sequences=True, input_shape = x_train[0].shape)))\n",
    "regressor.add(Dropout(dropout))\n",
    "\n",
    "regressor.add(LSTM(units = n_past-1, return_sequences = True))\n",
    "regressor.add(Dropout(dropout))\n",
    "\n",
    "regressor.add(LSTM(units = n_past-2, return_sequences = False))\n",
    "regressor.add(Dropout(dropout))\n",
    "\n",
    "regressor.add(LSTM(units = n_past, return_sequences = True))\n",
    "regressor.add(Dropout(dropout))\n",
    "\n",
    "\n",
    "regressor.add(Dense(units = n_future, activation = 'linear'))\n",
    "'''\n",
    "\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics = ['acc'])\n",
    "\n",
    "regressor.fit(x_train, y_train, epochs = 5, batch_size = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f3f52a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "This follows the general layout of https://keras.io/examples/vision/conv_lstm/. I haven't had much success with it\n",
    "    \n",
    "<b>Train your RNN:</b> Train your RNN using the training data.\n",
    "    \n",
    "    You might notice a lot of arbitrary values here (these are things we will want to change and test)\n",
    "    - Dropout: High dropout leads to more generalization. Low dropout takes advantage of more data but overfits more easily\n",
    "    - # of hidden layers: Right now, there are 2 hidden layers (+ 1 at the end). We can change this\n",
    "    - # units at hidden layers: Right now, units decrease by 1 at each hidden layer. This is arbitrary and can be changed\n",
    "    - optimizer --> 'adam' works but I don't know what it does\n",
    "    - epochs --> We can raise this above 1 but I don't notice that changing much when I have done so\n",
    "    - batch_size --> Higher batch size leads to more generalization\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "4838d735",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = x_train.reshape((int(408 * 256**2/d**2), 4, d, d, 1)), y_train.reshape((int(408 * 256**2/d**2), d, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ae547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to run this if you are loading a model\n",
    "regressor = Sequential()\n",
    "\n",
    "dropout = 0.1\n",
    "# Construct the input layer with no definite frame size.\n",
    "inp = tf.keras.layers.Input(shape=(None, *x_train.shape[2:]))\n",
    "#regressor.add(tf.keras.layers.Masking(mask_value=-9999,input_shape= x_train[0].shape)) \n",
    "x = tf.keras.layers.ConvLSTM2D(filters = 64, kernel_size = (5,5), padding=\"same\", return_sequences = True, activation = \"relu\")(inp)\n",
    "\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "x = tf.keras.layers.ConvLSTM2D(\n",
    "    filters=64,\n",
    "    kernel_size=(3, 3),\n",
    "    padding=\"same\",\n",
    "    return_sequences=True,\n",
    "    activation=\"relu\",\n",
    ")(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.ConvLSTM2D(\n",
    "    filters=64,\n",
    "    kernel_size=(1, 1),\n",
    "    padding=\"same\",\n",
    "    return_sequences=True,\n",
    "    activation=\"relu\",\n",
    ")(x)\n",
    "x = tf.keras.layers.ConvLSTM2D(\n",
    "    filters=1,\n",
    "    kernel_size=(1, 1),\n",
    "    padding=\"same\",\n",
    "    return_sequences=True,\n",
    "    activation=\"relu\",\n",
    ")(x)\n",
    "\n",
    "'''\n",
    "x = tf.keras.layers.Conv3D(\n",
    "    filters=1, kernel_size=(3, 3, 3), activation=\"sigmoid\", padding=\"same\"\n",
    ")(x)\n",
    "'''\n",
    "\n",
    "model = tf.keras.models.Model(inp, x)\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.MeanAbsoluteError(), optimizer=tf.keras.optimizers.Adam(),\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=1,\n",
    "    epochs=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d3f600",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Load pre-trained model:</b> \n",
    "    If you want to load a model instead, use this\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "530f10b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model_name = 'cnn_lstm_1' ## CHANGE THIS (e.g. 'my_model') ##\n",
    "regressor = load_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b61cb40",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Save pre-trained model:</b> \n",
    "    If you want to save a model instead, use this\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "908cd644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_lstm_keras_2\\assets\n"
     ]
    }
   ],
   "source": [
    "model_name = 'cnn_lstm_keras_2'## CHANGE THIS (e.g. 'my_model') ##\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dbe01f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Input Data for Testing:</b> \n",
    "    Below, we get the input data from March 2018! \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "0c05eacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "path = path = \"C:\\\\Users\\\\nico\\\\Desktop\\\\Stanford\\\\OneDrive - Stanford\\\\Courses\\\\CS229\\\\finalproject\\\\data\\\\United_States_Fires\\\\United_States_2018_Fires\\\\nov\\\\storage\"\n",
    "x_test, y_test, pca_array  = nn_train_test(path, n_past = 3, n_comp = n_comp, n_future =1, d = d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f40225d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Check:</b> I've noticed the first dimensions of these sometimes these don't match. They should.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "577fa0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087bb6ec",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b> Predict Step:</b> This makes your predictions using your model and reshapes them into a list of 2D arrays\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "e5a41234",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "predict_test = regressor.predict(x_test)\n",
    "p = predict_test\n",
    "shape = predict_test.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
