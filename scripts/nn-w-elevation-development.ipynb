{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for development! Use it to try out new strategies :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Imports:</b> Common imports for NumPy, MatplotLib, Pandas, and OS </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import for data generation\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>ML Imports:</b> Imports to execute RNN model for sequential data </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicoc\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\nicoc\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\nicoc\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "C:\\Users\\nicoc\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\nicoc\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\nicoc\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "C:\\Users\\nicoc\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\nicoc\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\nicoc\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "C:\\Users\\nicoc\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:29: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n",
      "C:\\Users\\nicoc\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "import tensorflow as tf\n",
    "from pyspark import SparkContext\n",
    "from sklearn import decomposition\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>nn_train_test() function:</b> \n",
    "    \n",
    "This function creates the train and test sets a little differently. Instead of using pca to reduce dimensionality, it keeps all the data in anticipation of the Neural Net Framework to deal with filtering through it. The d variable allows you to discretize the grid down further as well.\n",
    "    \n",
    "    path --> path to the repository with all your .npy file\n",
    "    n_future --> number of future days of fire you want to predict (will be 1 for us)\n",
    "    n_comp --> Doesn't do anything right now\n",
    "    n_past --> number of past days you want to consider (we can experiment with this)\n",
    "    d --> The size of the discretized fires you want (i.e. d= 32, returns subset of sequences for 32x32 grids)\n",
    "    \n",
    "Outputs:\n",
    "    \n",
    "    input_dim is set to a 1-D array of 256 ^ 2 for now! \n",
    "    x_train --> x_train in format (batch_size, timesteps, input_dim)\n",
    "    y_train --> training data in format (batch_size, input_dim)\n",
    "    pca_array -->  Nothing\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_train_test(path, n_past, n_comp, n_future = 1, d = 256):\n",
    "    x_train_list = []\n",
    "    y_train_list = []\n",
    "    count = 0\n",
    "    for file in os.listdir(r\"{path}\".format(path = path)):\n",
    "        if os.path.getsize(\"{path}\\\\{file}\".format(path = path, file = file)) < 10000000:\n",
    "            continue\n",
    "        with open(\"{path}\\\\{file}\".format(path = path, file = file), 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        training_fire = data['multiDay']\n",
    "        elevation_data = data['Elevation Data'][:-1,:-1]\n",
    "        elev_shape = np.array(elevation_data.shape)\n",
    "        elevation_fire = np.zeros((256, 256))\n",
    "        shape = np.array(training_fire.shape) \n",
    "        fire = np.where(training_fire[1] == 1)\n",
    "        x_center, y_center = int(np.median(fire[0])), int(np.median(fire[1]))\n",
    "        if shape[0] < n_past + n_future:\n",
    "            continue\n",
    "        standard_fire = np.zeros((len(training_fire), 256, 256))    \n",
    "        if shape[1] > 255 or shape[2] > 255:\n",
    "\n",
    "\n",
    "            # could not broadcast input array from shape (39,350,325) into shape (39,512,416)\n",
    "            xLow = x_center - 128 \n",
    "            xHi = shape[1] - x_center\n",
    "            yLow = y_center - 128\n",
    "            yHi = shape[2] - y_center\n",
    "            print(xLow, xHi, yLow, yHi)\n",
    "            if shape[1] > 255:\n",
    "                if xLow < 0:\n",
    "                    xHi = x_center + 128 - xLow\n",
    "                    xLow = 0\n",
    "                elif xHi < 128:\n",
    "                    xLow = x_center - 256 + xHi\n",
    "                    xHi = shape[1]\n",
    "                else:\n",
    "                    xLow = x_center - 128\n",
    "                    xHi = x_center + 128\n",
    "            else:\n",
    "                xLow = 0\n",
    "                xHi = shape[1]\n",
    "\n",
    "            if shape[2] > 255:\n",
    "                if yLow < 0:\n",
    "                    yHi = y_center + 128 - yLow\n",
    "                    yLow = 0\n",
    "                elif yHi < 128:\n",
    "                    yLow = y_center - 256 + yHi\n",
    "                    yHi = shape[2]\n",
    "                else:\n",
    "                    yLow = y_center - 128\n",
    "                    yHi = y_center + 128\n",
    "            else:\n",
    "                yLow = 0\n",
    "                yHi = shape[2]\n",
    "            print(xLow, xHi, yLow, yHi)\n",
    "            standard_fire[:shape[0], :min(256, shape[1]), :min(256, shape[2])] = training_fire[:, xLow: xHi, yLow: yHi]   \n",
    "            elevation_fire[:min(256, elev_shape[0]), :min(256, elev_shape[1])] = elevation_data[xLow: xHi, yLow: yHi]\n",
    "        else: \n",
    "            standard_fire[:shape[0], :shape[1], :shape[2]] = training_fire\n",
    "            elevation_fire[:min(256, elev_shape[0]), :min(256, elev_shape[1])] = elevation_data\n",
    "\n",
    "        pca_fire = standard_fire\n",
    "        pca_elev = elevation_fire.reshape((1, 256, 256))\n",
    "        for i in range(0 , len(training_fire) - n_future - n_past + 1):\n",
    "            shape = pca_fire[i : i + n_past].shape\n",
    "            a = np.zeros((shape[0] + 1, shape[1], shape[2]))\n",
    "            a[:3] = pca_fire[i : i + n_past]\n",
    "            a[3:] = pca_elev \n",
    "            for j in np.arange(0, 255, d):\n",
    "                for k in np.arange(0, 255, d):\n",
    "                    x_train_list.append(a[:, j:j+d, k:k+d])\n",
    "                    y_train_list.append(pca_fire[i + n_past: i + n_past + n_future, j:j+d, k:k+d])\n",
    "    \n",
    "    x_train = np.array(x_train_list)\n",
    "    y_train = np.array(y_train_list)  \n",
    "\n",
    "    print(x_train.shape, y_train.shape)\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>RNN Inputs:</b> \n",
    "We are declaring below that the number of future days we want to predict is 1, and we can vary the number of past days\n",
    "    we want to predict (I have the default as 3 for now)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_future = 1\n",
    "n_comp = 10\n",
    "n_past = 3\n",
    "discretization_amount = d = 64 # 256 means we keep the whole fire grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Input Data for Training:</b> \n",
    "    Below, we get the input data from December 2018! \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture \n",
    "# We are suppressing print statements and warning messages w/ above line\n",
    "path = \"C:\\\\Users\\\\nicoc\\\\Desktop\\\\Stanford\\\\OneDrive\\\\OneDrive - Stanford\\\\Courses\\\\CS229\\\\finalproject\\\\data\\\\United_States_Fires\\\\United_States_2017_Fires\\\\jan\\\\storage\"\n",
    "months = ['feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug' ,'sep', 'oct', 'nov', 'dec']\n",
    "x_train, y_train = nn_train_test(path, n_past = n_past, n_comp = n_comp, n_future = n_future, d = d)\n",
    "for month in months:\n",
    "    path = \"C:\\\\Users\\\\nicoc\\\\Desktop\\\\Stanford\\\\OneDrive\\\\OneDrive - Stanford\\\\Courses\\\\CS229\\\\finalproject\\\\data\\\\United_States_Fires\\\\United_States_2017_Fires\\\\{mon}\\\\storage\".format(mon = month)\n",
    "    x, y  = nn_train_test(path, n_past = n_past, n_comp = n_comp, n_future = n_future, d = d)\n",
    "    if len(x) == 0:\n",
    "        continue\n",
    "    x_train = np.vstack((x_train, x))\n",
    "    y_train = np.vstack((y_train, y))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "# We are suppressing print statements and warning messages w/ above line\n",
    "path = \"C:\\\\Users\\\\nicoc\\\\Desktop\\\\Stanford\\\\OneDrive\\\\OneDrive - Stanford\\\\Courses\\\\CS229\\\\finalproject\\\\data\\\\United_States_Fires\\\\United_States_2018_Fires\\\\dec\\\\storage\"\n",
    "months = ['feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug' ,'sep', 'oct', 'nov', 'dec']\n",
    "x_train, y_train = nn_train_test(path, n_past = n_past, n_comp = n_comp, n_future = n_future, d = d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Check:</b> \n",
    "    Check the shape of your x_train and y_train data\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    This follows the model in https://towardsdatascience.com/wildfire-spreading-modeling-in-alberta-canada-a-trial-using-a-neural-network-with-convlstm-cells-81c1a9f7d410. Their GitHub is at the bottom of the page \n",
    "    \n",
    "<b>Train your RNN:</b> Train your RNN using the training data.\n",
    "    \n",
    "    You might notice a lot of arbitrary values here (these are things we will want to change and test)\n",
    "    - Dropout: High dropout leads to more generalization. Low dropout takes advantage of more data but overfits more easily\n",
    "    - # of hidden layers: Right now, there are 2 hidden layers (+ 1 at the end). We can change this\n",
    "    - # units at hidden layers: Right now, units decrease by 1 at each hidden layer. This is arbitrary and can be changed\n",
    "    - optimizer --> 'adam' works but I don't know what it does\n",
    "    - epochs --> We can raise this above 1 but I don't notice that changing much when I have done so\n",
    "    - batch_size --> Higher batch size leads to more generalization\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape your data as needed for the ConvLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = x_train.reshape((int(x_train.shape[0]), d, d, 4)), y_train.reshape((int(y_train.shape[0]), d*d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train = np.array([np.sum(y) for y in y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[:, :, :, 3][x_train[:, :, :, 3] == -9999] = 0\n",
    "for i in range(len(x_train)):\n",
    "    if x_train[i, :, :, 3].sum() > 0:\n",
    "        x_train[i,:,:,3] = x_train[i,:,:,3] / x_train[i,:,:,3].sum()\n",
    "    else:\n",
    "        x_train[i, :, :, 3] = np.ones(x_train[i, :, :, 3].shape) / x_train[i, :, :, 3].size\n",
    "        #print(x_train[i, :, :, 3])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to run this if you are loading a model\n",
    "regressor = Sequential()\n",
    "\n",
    "dropout = 0.1\n",
    "\n",
    "#regressor.add(tf.keras.layers.Masking(mask_value=-9999,input_shape= x_train[0].shape)) \n",
    "\n",
    "regressor.add(tf.keras.layers.Conv2D(filters = d, kernel_size = (3,3), padding = \"same\", activation = \"tanh\", input_shape = x_train[0].shape))\n",
    "\n",
    "\n",
    "regressor.add(tf.keras.layers.MaxPool2D(pool_size = (2,2)))\n",
    "\n",
    "\n",
    "regressor.add(tf.keras.layers.Conv2D(filters = 2*d, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
    "\n",
    "regressor.add(tf.keras.layers.MaxPool2D(pool_size = (2,2)))\n",
    "\n",
    "\n",
    "regressor.add(tf.keras.layers.Flatten())\n",
    "\n",
    "regressor.add(tf.keras.layers.Dense(units = 4*d, activation = 'relu'))\n",
    "\n",
    "regressor.add(tf.keras.layers.Dropout(0.1))\n",
    "\n",
    "regressor.add(tf.keras.layers.Dense(units = 8*d, activation = 'relu'))\n",
    "\n",
    "regressor.add(tf.keras.layers.Dropout(0.1))\n",
    "\n",
    "regressor.add(tf.keras.layers.Dense(units = 16*d, activation = 'relu'))\n",
    "\n",
    "regressor.add(tf.keras.layers.Dropout(0.1))\n",
    "\n",
    "regressor.add(tf.keras.layers.Dense(units = 32*d, activation = 'relu'))\n",
    "\n",
    "regressor.add(tf.keras.layers.Dropout(0.1))\n",
    "\n",
    "regressor.add(tf.keras.layers.Dense(units = d*d, activation = 'relu'))\n",
    "\n",
    "'''\n",
    "regressor.add(tf.keras.layers.Bidirectional(LSTM(units=n_past, return_sequences=True, input_shape = x_train[0].shape)))\n",
    "regressor.add(Dropout(dropout))\n",
    "\n",
    "regressor.add(LSTM(units = n_past-1, return_sequences = True))\n",
    "regressor.add(Dropout(dropout))\n",
    "\n",
    "regressor.add(LSTM(units = n_past-2, return_sequences = False))\n",
    "regressor.add(Dropout(dropout))\n",
    "\n",
    "regressor.add(LSTM(units = n_past, return_sequences = True))\n",
    "regressor.add(Dropout(dropout))\n",
    "\n",
    "\n",
    "regressor.add(Dense(units = n_future, activation = 'linear'))\n",
    "'''\n",
    "\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics = ['acc'])\n",
    "\n",
    "regressor.fit(x_train, y_train, validation_split=0.2, epochs = 1, batch_size = 10)\n",
    "\n",
    "model_name = '2018-num-of-fires'## CHANGE THIS (e.g. 'my_model') ##\n",
    "regressor.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, y_train.shape\n",
    "np.where(y_train > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ex = np.ones((2,4096))\n",
    "for i in range(2):   \n",
    "    ytr = y_train[i + 6]\n",
    "    for j, y in enumerate(ytr):\n",
    "        if y:\n",
    "            continue\n",
    "        else:\n",
    "            y_ex[i][j] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to run this if you are loading a model\n",
    "regressor = Sequential()\n",
    "\n",
    "regressor.add(tf.keras.layers.Conv2D(filters = d, kernel_size = (3,3), padding = \"same\", activation = \"tanh\", input_shape = x_train[0].shape))\n",
    "\n",
    "regressor.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "regressor.add(tf.keras.layers.MaxPool2D(pool_size = (2,2)))\n",
    "\n",
    "regressor.add(tf.keras.layers.Flatten())\n",
    "\n",
    "regressor.add(tf.keras.layers.Dense(units = d, activation = 'relu'))\n",
    "\n",
    "regressor.add(tf.keras.layers.Dense(units = d*d, activation = 'relu'))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "\n",
    "regressor.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['acc'])\n",
    "'''\n",
    "my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=2),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "]\n",
    "'''\n",
    "#regressor.fit(x_train[6].reshape((1, d, d, 4)), y_train[6].reshape((1,d*d)), epochs = 10, batch_size = 10)\n",
    "regressor.fit(x_train[6:8].reshape(2, d, d, 4), y_train[6:8].reshape((2, d*d)), epochs = 1, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14432, 1, 64, 64, 4), (14432, 4096))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train = x_train.reshape((int(x_train.shape[0]), 1, d, d, 4)), y_train.reshape((int(y_train.shape[0]), d*d))\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 2s 256ms/step - loss: 0.8659 - acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 1s 268ms/step - loss: 0.3192 - acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 224ms/step - loss: 0.2066 - acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 198ms/step - loss: 0.0812 - acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 224ms/step - loss: 0.0667 - acc: 0.0000e+00\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 1s 255ms/step - loss: 0.1426 - acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 207ms/step - loss: 0.0667 - acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 1s 226ms/step - loss: 0.0830 - acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 196ms/step - loss: 0.0778 - acc: 0.0000e+00\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 259ms/step - loss: 0.0635 - acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 228ms/step - loss: 0.0895 - acc: 0.0000e+00\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 1s 253ms/step - loss: 0.0223 - acc: 0.0000e+00\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 214ms/step - loss: 0.0238 - acc: 0.0000e+00\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 205ms/step - loss: 0.0212 - acc: 0.0000e+00\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 208ms/step - loss: 0.0098 - acc: 0.0000e+00\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 284ms/step - loss: 0.0230 - acc: 0.0000e+00\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 194ms/step - loss: 0.0043 - acc: 0.0000e+00\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 221ms/step - loss: 0.0137 - acc: 0.0000e+00\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 257ms/step - loss: 0.0024 - acc: 0.0000e+00\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 217ms/step - loss: 0.0112 - acc: 0.0000e+00\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 209ms/step - loss: 0.0065 - acc: 0.0000e+00\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 1s 262ms/step - loss: 0.0314 - acc: 0.0000e+00\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 210ms/step - loss: 0.0476 - acc: 0.0000e+00\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 201ms/step - loss: 0.0431 - acc: 0.0000e+00\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 1s 253ms/step - loss: 0.0042 - acc: 0.0000e+00\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 249ms/step - loss: 0.0369 - acc: 0.0000e+00\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 239ms/step - loss: 0.0045 - acc: 0.0000e+00\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 193ms/step - loss: 0.0051 - acc: 0.0000e+00\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 215ms/step - loss: 0.0033 - acc: 0.0000e+00\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 205ms/step - loss: 0.0033 - acc: 0.0000e+00\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 237ms/step - loss: 0.0022 - acc: 0.0000e+00\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 217ms/step - loss: 0.0012 - acc: 0.0000e+00\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 253ms/step - loss: 0.0023 - acc: 0.0000e+00\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 209ms/step - loss: 6.6867e-04 - acc: 0.0000e+00\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 249ms/step - loss: 0.0018 - acc: 0.0000e+00\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 228ms/step - loss: 0.0028 - acc: 0.0000e+00\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 1s 257ms/step - loss: 0.0011 - acc: 0.0000e+00\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 199ms/step - loss: 5.0524e-04 - acc: 0.0000e+00\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 251ms/step - loss: 0.0012 - acc: 0.0000e+00\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 227ms/step - loss: 4.8291e-04 - acc: 0.0000e+00\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 200ms/step - loss: 7.9485e-04 - acc: 0.0000e+00\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 210ms/step - loss: 0.0014 - acc: 0.0000e+00\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 242ms/step - loss: 7.9221e-04 - acc: 0.0000e+00\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 205ms/step - loss: 0.0150 - acc: 0.0000e+00\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 1s 258ms/step - loss: 0.0019 - acc: 0.0000e+00\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 1s 260ms/step - loss: 5.6270e-04 - acc: 0.0000e+00\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 1s 258ms/step - loss: 4.4738e-05 - acc: 0.0000e+00\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 1s 267ms/step - loss: 0.0081 - acc: 0.0000e+00\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 254ms/step - loss: 0.0034 - acc: 0.0000e+00\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 1s 273ms/step - loss: 0.0034 - acc: 0.0000e+00\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 1s 257ms/step - loss: 3.3068e-04 - acc: 0.0000e+00\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 1s 261ms/step - loss: 2.2727e-04 - acc: 0.0000e+00\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 1s 251ms/step - loss: 5.3510e-04 - acc: 0.0000e+00\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 1s 283ms/step - loss: 3.7329e-04 - acc: 0.0000e+00\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 1s 293ms/step - loss: 6.1617e-04 - acc: 0.0000e+00\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 1s 283ms/step - loss: 3.8946e-05 - acc: 0.0000e+00\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 1s 266ms/step - loss: 3.2245e-04 - acc: 0.0000e+00\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 239ms/step - loss: 2.3906e-04 - acc: 0.0000e+00\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 235ms/step - loss: 4.9194e-04 - acc: 0.0000e+00\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 231ms/step - loss: 2.0891e-04 - acc: 0.0000e+00\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 227ms/step - loss: 0.0030 - acc: 0.0000e+00\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 1s 248ms/step - loss: 7.1189e-05 - acc: 0.0000e+00\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 1s 240ms/step - loss: 1.5360e-04 - acc: 0.0000e+00\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 218ms/step - loss: 2.6344e-04 - acc: 0.0000e+00\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 231ms/step - loss: 3.5689e-04 - acc: 0.0000e+00\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 248ms/step - loss: 0.0018 - acc: 0.0000e+00\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 237ms/step - loss: 4.2332e-04 - acc: 0.0000e+00\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 241ms/step - loss: 0.0010 - acc: 0.0000e+00\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 224ms/step - loss: 1.7091e-04 - acc: 0.0000e+00\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 252ms/step - loss: 5.1624e-04 - acc: 0.0000e+00\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 226ms/step - loss: 6.1184e-04 - acc: 0.0000e+00\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 233ms/step - loss: 4.9882e-04 - acc: 0.0000e+00\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 214ms/step - loss: 9.0056e-05 - acc: 0.0000e+00\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 215ms/step - loss: 1.1912e-04 - acc: 0.0000e+00\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 223ms/step - loss: 3.4237e-04 - acc: 0.0000e+00\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 222ms/step - loss: 8.1657e-04 - acc: 0.0000e+00\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 1s 253ms/step - loss: 6.5776e-05 - acc: 0.0000e+00\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 246ms/step - loss: 1.8889e-04 - acc: 0.0000e+00\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 234ms/step - loss: 1.6126e-04 - acc: 0.0000e+00\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 215ms/step - loss: 1.7768e-04 - acc: 0.0000e+00\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 1s 269ms/step - loss: 6.4708e-05 - acc: 0.0000e+00\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 228ms/step - loss: 8.8023e-05 - acc: 0.0000e+00\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 225ms/step - loss: 4.5956e-04 - acc: 0.0000e+00\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 233ms/step - loss: 3.5187e-05 - acc: 0.0000e+00\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 254ms/step - loss: 1.7677e-04 - acc: 0.0000e+00\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 221ms/step - loss: 2.3941e-04 - acc: 0.0000e+00\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 248ms/step - loss: 0.0011 - acc: 0.0000e+00\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 245ms/step - loss: 4.3528e-05 - acc: 0.0000e+00\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 214ms/step - loss: 1.7150e-04 - acc: 0.0000e+00\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 227ms/step - loss: 0.0016 - acc: 0.0000e+00\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 262ms/step - loss: 4.4616e-04 - acc: 0.0000e+00\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 235ms/step - loss: 0.0023 - acc: 0.0000e+00\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 1s 276ms/step - loss: 2.0261e-04 - acc: 0.0000e+00\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 249ms/step - loss: 1.1851e-04 - acc: 0.0000e+00\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 254ms/step - loss: 2.3460e-04 - acc: 0.0000e+00\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 242ms/step - loss: 9.8442e-06 - acc: 0.0000e+00\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 216ms/step - loss: 3.6516e-05 - acc: 0.0000e+00\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 223ms/step - loss: 2.6983e-04 - acc: 0.0000e+00\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 254ms/step - loss: 1.6339e-04 - acc: 0.0000e+00\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 221ms/step - loss: 5.4920e-04 - acc: 0.0000e+00\n",
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " max_pooling3d_54 (MaxPoolin  (1, 1, 32, 32, 4)        0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_54 (Bat  (1, 1, 32, 32, 4)        16        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_36 (Dropout)        (1, 1, 32, 32, 4)         0         \n",
      "                                                                 \n",
      " max_pooling3d_55 (MaxPoolin  (1, 1, 16, 16, 4)        0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_55 (Bat  (1, 1, 16, 16, 4)        16        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        (1, 1, 16, 16, 4)         0         \n",
      "                                                                 \n",
      " conv3d_177 (Conv3D)         (1, 1, 14, 14, 256)       9472      \n",
      "                                                                 \n",
      " max_pooling3d_56 (MaxPoolin  (1, 1, 7, 7, 256)        0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_56 (Bat  (1, 1, 7, 7, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " flatten_17 (Flatten)        (1, 12544)                0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (1, 4096)                 51384320  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 51,394,848\n",
      "Trainable params: 51,394,320\n",
      "Non-trainable params: 528\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# No need to run this if you are loading a model\n",
    "regressor = Sequential()\n",
    "\n",
    "act = \"tanh\"\n",
    "batch_normalization = True\n",
    "layersd = 0\n",
    "layers2d = 0\n",
    "layers4d = 1\n",
    "pooling = True ## DO NOT CHANGE\n",
    "dropout = 0.1\n",
    "learning_rate = .001\n",
    "epochs = 100\n",
    "dense = 1\n",
    "\n",
    "for i in range(layersd):\n",
    "    regressor.add(tf.keras.layers.Conv3D(filters = d, kernel_size = (1,3,3), padding = \"valid\", activation = act, input_shape = x_train[0].shape))\n",
    "\n",
    "if pooling:\n",
    "    regressor.add(tf.keras.layers.MaxPooling3D(pool_size=(1, 2, 2), padding=\"valid\"))\n",
    "\n",
    "if batch_normalization:\n",
    "    regressor.add(tf.keras.layers.BatchNormalization(center=True, scale=True))\n",
    "\n",
    "regressor.add(Dropout(dropout))\n",
    "\n",
    "for i in range(layers2d):\n",
    "    regressor.add(tf.keras.layers.Conv3D(filters = d*2, kernel_size = (1,3,3), padding = \"valid\", activation = act))\n",
    "\n",
    "if pooling:\n",
    "    regressor.add(tf.keras.layers.MaxPooling3D(pool_size=(1, 2, 2), padding=\"valid\"))\n",
    "    \n",
    "if batch_normalization:\n",
    "    regressor.add(tf.keras.layers.BatchNormalization(center=True, scale=True))\n",
    "\n",
    "regressor.add(Dropout(dropout))\n",
    "\n",
    "for i in range(layers4d):\n",
    "    regressor.add(tf.keras.layers.Conv3D(filters = d*4, kernel_size = (1,3,3), padding = \"valid\", activation = act))\n",
    "\n",
    "if pooling:\n",
    "    regressor.add(tf.keras.layers.MaxPooling3D(pool_size=(1, 2, 2), padding=\"valid\"))\n",
    "\n",
    "if batch_normalization:\n",
    "    regressor.add(tf.keras.layers.BatchNormalization(center=True, scale=True))\n",
    "\n",
    "regressor.add(tf.keras.layers.Flatten())\n",
    "\n",
    "for i in range(dense):\n",
    "    regressor.add(tf.keras.layers.Dense(units = d*d, activation = 'sigmoid'))\n",
    "\n",
    "\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "regressor.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['acc'])\n",
    "#regressor.compile(optimizer = opt, loss = 'mse', metrics = ['acc'])\n",
    "'''\n",
    "my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=2),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "]\n",
    "'''\n",
    "#regressor.fit(x_train[6].reshape((1, d, d, 4)), y_train[6].reshape((1,d*d)), epochs = 10, batch_size = 10)\n",
    "regressor.fit(x_train[37:39].reshape(2, 1, d, d, 4), y_train[37:39].reshape((2, d*d)), epochs = epochs, batch_size = 1)\n",
    "regressor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(310.31177, 318.99188, 319.0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(b), np.sum(p), np.sum(y_train[38])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2729b5d7d48>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_test = regressor.predict(x_train[37].reshape((1, 1, d, d, 4)))\n",
    "b = predict_test\n",
    "plt.matshow(b.reshape((64,64)))\n",
    "predict_test = regressor.predict(x_train[38].reshape((1, 1, d, d, 4)))\n",
    "p = predict_test\n",
    "plt.matshow(p.reshape((64,64)))\n",
    "plt.matshow(y_train[37].reshape((64,64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1883.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
       "       25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
       "       25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 31, 31, 31, 31, 31, 31, 31,\n",
       "       31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n",
       "       31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n",
       "       31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n",
       "       31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n",
       "       31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n",
       "       31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n",
       "       31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n",
       "       31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n",
       "       31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n",
       "       31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n",
       "       31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 36, 36, 36, 36, 36, 36,\n",
       "       36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36,\n",
       "       36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36,\n",
       "       36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 38, 38, 38, 38, 38, 38, 38,\n",
       "       38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
       "       38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
       "       38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
       "       38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
       "       38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
       "       38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
       "       38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
       "       38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
       "       38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
       "       38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
       "       38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
       "       38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
       "       38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
       "       38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
       "       38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
       "       38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
       "       38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
       "       38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
       "       38, 38, 38, 38, 38, 38, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41,\n",
       "       41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41,\n",
       "       41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41,\n",
       "       41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41,\n",
       "       41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(y_train>0)[0][1000:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "This follows the general layout of https://keras.io/examples/vision/conv_lstm/. I haven't had much success with it\n",
    "    \n",
    "<b>Train your RNN:</b> Train your RNN using the training data.\n",
    "    \n",
    "    You might notice a lot of arbitrary values here (these are things we will want to change and test)\n",
    "    - Dropout: High dropout leads to more generalization. Low dropout takes advantage of more data but overfits more easily\n",
    "    - # of hidden layers: Right now, there are 2 hidden layers (+ 1 at the end). We can change this\n",
    "    - # units at hidden layers: Right now, units decrease by 1 at each hidden layer. This is arbitrary and can be changed\n",
    "    - optimizer --> 'adam' works but I don't know what it does\n",
    "    - epochs --> We can raise this above 1 but I don't notice that changing much when I have done so\n",
    "    - batch_size --> Higher batch size leads to more generalization\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = x_train.reshape((int(408 * 256**2/d**2), 4, d, d, 1)), y_train.reshape((int(408 * 256**2/d**2), d, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to run this if you are loading a model\n",
    "regressor = Sequential()\n",
    "\n",
    "dropout = 0.1\n",
    "# Construct the input layer with no definite frame size.\n",
    "inp = tf.keras.layers.Input(shape=(None, *x_train.shape[2:]))\n",
    "#regressor.add(tf.keras.layers.Masking(mask_value=-9999,input_shape= x_train[0].shape)) \n",
    "x = tf.keras.layers.ConvLSTM2D(filters = 64, kernel_size = (5,5), padding=\"same\", return_sequences = True, activation = \"relu\")(inp)\n",
    "\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "x = tf.keras.layers.ConvLSTM2D(\n",
    "    filters=64,\n",
    "    kernel_size=(3, 3),\n",
    "    padding=\"same\",\n",
    "    return_sequences=True,\n",
    "    activation=\"relu\",\n",
    ")(x)\n",
    "\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.ConvLSTM2D(\n",
    "    filters=64,\n",
    "    kernel_size=(1, 1),\n",
    "    padding=\"same\",\n",
    "    return_sequences=True,\n",
    "    activation=\"relu\",\n",
    ")(x)\n",
    "x = tf.keras.layers.ConvLSTM2D(\n",
    "    filters=1,\n",
    "    kernel_size=(1, 1),\n",
    "    padding=\"same\",\n",
    "    return_sequences=True,\n",
    "    activation=\"relu\",\n",
    ")(x)\n",
    "\n",
    "'''\n",
    "x = tf.keras.layers.Conv3D(\n",
    "    filters=1, kernel_size=(3, 3, 3), activation=\"sigmoid\", padding=\"same\"\n",
    ")(x)\n",
    "'''\n",
    "\n",
    "model = tf.keras.models.Model(inp, x)\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.MeanAbsoluteError(), optimizer=tf.keras.optimizers.Adam(),\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=1,\n",
    "    epochs=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Load pre-trained model:</b> \n",
    "    If you want to load a model instead, use this\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model_name = '' ## CHANGE THIS (e.g. 'my_model') ##\n",
    "regressor = load_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Save pre-trained model:</b> \n",
    "    If you want to save a model instead, use this\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '1d-output'## CHANGE THIS (e.g. 'my_model') ##\n",
    "regressor.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Input Data for Testing:</b> \n",
    "    Below, we get the input data from March 2018! \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "path = path = \"C:\\\\Users\\\\nicoc\\\\Desktop\\\\Stanford\\\\OneDrive\\\\OneDrive - Stanford\\\\Courses\\\\CS229\\\\finalproject\\\\data\\\\United_States_Fires\\\\United_States_2018_Fires\\\\nov\\\\storage\"\n",
    "x_test, y_test  = nn_train_test(path, n_past = 3, n_comp = n_comp, n_future =1, d = d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Check:</b> I've noticed the first dimensions of these sometimes these don't match. They should.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = x_test.reshape((x_test.shape[0], d, d, 4)), y_test.reshape((int(y_test.shape[0]), d, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array([np.sum(y) for y in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test[:, :, :, 3][x_test[:, :, :, 3] == -9999] = 0\n",
    "for i in range(len(x_test)):\n",
    "    if x_test[i,:,:,3].sum() > 0:\n",
    "        x_test[i,:,:,3] = x_test[i,:,:,3] / x_test[i,:,:,3].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b> Predict Step:</b> This makes your predictions using your model and reshapes them into a list of 2D arrays\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "predict_test = regressor.predict(x_test)\n",
    "p = predict_test\n",
    "shape = predict_test.shape\n",
    "#p = p.reshape((p.shape[0], d, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 6\n",
    "p[k], y_test[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(p[3].reshape((64,64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_train = regressor.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_train, 'blue')\n",
    "plt.plot(predict_train, 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(y_test, 'blue')\n",
    "plt.plot(p, 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array([np.sum(y) for y in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test[599,:,:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(y_test[666])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(p[60])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
