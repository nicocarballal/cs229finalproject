{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for development! Use it to try out new strategies :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Imports:</b> Common imports for NumPy, MatplotLib, Pandas, and OS </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import for data generation\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>ML Imports:</b> Imports to execute RNN model for sequential data </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "import tensorflow as tf\n",
    "from pyspark import SparkContext\n",
    "from sklearn import decomposition\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>nn_train_test() function:</b> \n",
    "    \n",
    "This function creates the train and test sets a little differently. Instead of using pca to reduce dimensionality, it keeps all the data in anticipation of the Neural Net Framework to deal with filtering through it. The d variable allows you to discretize the grid down further as well.\n",
    "    \n",
    "    path --> path to the repository with all your .npy file\n",
    "    n_future --> number of future days of fire you want to predict (will be 1 for us)\n",
    "    n_comp --> Doesn't do anything right now\n",
    "    n_past --> number of past days you want to consider (we can experiment with this)\n",
    "    d --> The size of the discretized fires you want (i.e. d= 32, returns subset of sequences for 32x32 grids)\n",
    "    \n",
    "Outputs:\n",
    "    \n",
    "    input_dim is set to a 1-D array of 256 ^ 2 for now! \n",
    "    x_train --> x_train in format (batch_size, timesteps, input_dim)\n",
    "    y_train --> training data in format (batch_size, input_dim)\n",
    "    pca_array -->  Nothing\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_train_test(path, n_past, n_comp, n_future = 1, d = 256):\n",
    "    x_train_list = []\n",
    "    y_train_list = []\n",
    "    count = 0\n",
    "    for file in os.listdir(r\"{path}\".format(path = path)):\n",
    "        if os.path.getsize(\"{path}\\\\{file}\".format(path = path, file = file)) < 10000000:\n",
    "            continue\n",
    "        with open(\"{path}\\\\{file}\".format(path = path, file = file), 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        training_fire = data['multiDay']\n",
    "        elevation_data = data['Elevation Data'][:-1,:-1]\n",
    "        elev_shape = np.array(elevation_data.shape)\n",
    "        elevation_fire = np.zeros((256, 256))\n",
    "        shape = np.array(training_fire.shape) \n",
    "        fire = np.where(training_fire[1] == 1)\n",
    "        x_center, y_center = int(np.median(fire[0])), int(np.median(fire[1]))\n",
    "        if shape[0] < n_past + n_future:\n",
    "            continue\n",
    "        standard_fire = np.zeros((len(training_fire), 256, 256))    \n",
    "        if shape[1] > 255 or shape[2] > 255:\n",
    "\n",
    "\n",
    "            # could not broadcast input array from shape (39,350,325) into shape (39,512,416)\n",
    "            xLow = x_center - 128 \n",
    "            xHi = shape[1] - x_center\n",
    "            yLow = y_center - 128\n",
    "            yHi = shape[2] - y_center\n",
    "            print(xLow, xHi, yLow, yHi)\n",
    "            if shape[1] > 255:\n",
    "                if xLow < 0:\n",
    "                    xHi = x_center + 128 - xLow\n",
    "                    xLow = 0\n",
    "                elif xHi < 128:\n",
    "                    xLow = x_center - 256 + xHi\n",
    "                    xHi = shape[1]\n",
    "                else:\n",
    "                    xLow = x_center - 128\n",
    "                    xHi = x_center + 128\n",
    "            else:\n",
    "                xLow = 0\n",
    "                xHi = shape[1]\n",
    "\n",
    "            if shape[2] > 255:\n",
    "                if yLow < 0:\n",
    "                    yHi = y_center + 128 - yLow\n",
    "                    yLow = 0\n",
    "                elif yHi < 128:\n",
    "                    yLow = y_center - 256 + yHi\n",
    "                    yHi = shape[2]\n",
    "                else:\n",
    "                    yLow = y_center - 128\n",
    "                    yHi = y_center + 128\n",
    "            else:\n",
    "                yLow = 0\n",
    "                yHi = shape[2]\n",
    "            print(xLow, xHi, yLow, yHi)\n",
    "            standard_fire[:shape[0], :min(256, shape[1]), :min(256, shape[2])] = training_fire[:, xLow: xHi, yLow: yHi]   \n",
    "            elevation_fire[:min(256, elev_shape[0]), :min(256, elev_shape[1])] = elevation_data[xLow: xHi, yLow: yHi]\n",
    "        else: \n",
    "            standard_fire[:shape[0], :shape[1], :shape[2]] = training_fire\n",
    "            elevation_fire[:min(256, elev_shape[0]), :min(256, elev_shape[1])] = elevation_data\n",
    "\n",
    "        pca_fire = standard_fire\n",
    "        pca_elev = elevation_fire.reshape((1, 256, 256))\n",
    "        for i in range(0 , len(training_fire) - n_future - n_past + 1):\n",
    "            shape = pca_fire[i : i + n_past].shape\n",
    "            a = np.zeros((shape[0] + 1, shape[1], shape[2]))\n",
    "            a[:3] = pca_fire[i : i + n_past]\n",
    "            a[3:] = pca_elev \n",
    "            for j in np.arange(0, 255, d):\n",
    "                for k in np.arange(0, 255, d):\n",
    "                    x_train_list.append(a[:, j:j+d, k:k+d])\n",
    "                    y_train_list.append(pca_fire[i + n_past: i + n_past + n_future, j:j+d, k:k+d])\n",
    "    \n",
    "    x_train = np.array(x_train_list)\n",
    "    y_train = np.array(y_train_list)  \n",
    "\n",
    "    print(x_train.shape, y_train.shape)\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>RNN Inputs:</b> \n",
    "We are declaring below that the number of future days we want to predict is 1, and we can vary the number of past days\n",
    "    we want to predict (I have the default as 3 for now)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_future = 1\n",
    "n_comp = 10\n",
    "n_past = 3\n",
    "discretization_amount = d = 64 # 256 means we keep the whole fire grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Input Data for Training:</b> \n",
    "    Below, we get the input data from December 2018! \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture \n",
    "# We are suppressing print statements and warning messages w/ above line\n",
    "path = \"C:\\\\Users\\\\nicoc\\\\Desktop\\\\Stanford\\\\OneDrive\\\\OneDrive - Stanford\\\\Courses\\\\CS229\\\\finalproject\\\\data\\\\United_States_Fires\\\\United_States_2017_Fires\\\\jan\\\\storage\"\n",
    "months = ['feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug' ,'sep', 'oct', 'nov', 'dec']\n",
    "x_train, y_train = nn_train_test(path, n_past = n_past, n_comp = n_comp, n_future = n_future, d = d)\n",
    "for month in months:\n",
    "    path = \"C:\\\\Users\\\\nicoc\\\\Desktop\\\\Stanford\\\\OneDrive\\\\OneDrive - Stanford\\\\Courses\\\\CS229\\\\finalproject\\\\data\\\\United_States_Fires\\\\United_States_2017_Fires\\\\{mon}\\\\storage\".format(mon = month)\n",
    "    x, y  = nn_train_test(path, n_past = n_past, n_comp = n_comp, n_future = n_future, d = d)\n",
    "    if len(x) == 0:\n",
    "        continue\n",
    "    x_train = np.vstack((x_train, x))\n",
    "    y_train = np.vstack((y_train, y))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "# We are suppressing print statements and warning messages w/ above line\n",
    "path = \"C:\\\\Users\\\\nicoc\\\\Desktop\\\\Stanford\\\\OneDrive\\\\OneDrive - Stanford\\\\Courses\\\\CS229\\\\finalproject\\\\data\\\\United_States_Fires\\\\United_States_2018_Fires\\\\dec\\\\storage\"\n",
    "months = ['feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug' ,'sep', 'oct', 'nov', 'dec']\n",
    "x_train, y_train = nn_train_test(path, n_past = n_past, n_comp = n_comp, n_future = n_future, d = d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Check:</b> \n",
    "    Check the shape of your x_train and y_train data\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    This follows the model in https://towardsdatascience.com/wildfire-spreading-modeling-in-alberta-canada-a-trial-using-a-neural-network-with-convlstm-cells-81c1a9f7d410. Their GitHub is at the bottom of the page \n",
    "    \n",
    "<b>Train your RNN:</b> Train your RNN using the training data.\n",
    "    \n",
    "    You might notice a lot of arbitrary values here (these are things we will want to change and test)\n",
    "    - Dropout: High dropout leads to more generalization. Low dropout takes advantage of more data but overfits more easily\n",
    "    - # of hidden layers: Right now, there are 2 hidden layers (+ 1 at the end). We can change this\n",
    "    - # units at hidden layers: Right now, units decrease by 1 at each hidden layer. This is arbitrary and can be changed\n",
    "    - optimizer --> 'adam' works but I don't know what it does\n",
    "    - epochs --> We can raise this above 1 but I don't notice that changing much when I have done so\n",
    "    - batch_size --> Higher batch size leads to more generalization\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape your data as needed for the ConvLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = x_train.reshape((int(x_train.shape[0]), d, d, 4)), y_train.reshape((int(y_train.shape[0]), d*d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train = np.array([np.sum(y) for y in y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[:, :, :, 3][x_train[:, :, :, 3] == -9999] = 0\n",
    "for i in range(len(x_train)):\n",
    "    if x_train[i, :, :, 3].sum() > 0:\n",
    "        x_train[i,:,:,3] = x_train[i,:,:,3] / x_train[i,:,:,3].sum()\n",
    "    else:\n",
    "        x_train[i, :, :, 3] = np.ones(x_train[i, :, :, 3].shape) / x_train[i, :, :, 3].size\n",
    "        #print(x_train[i, :, :, 3])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to run this if you are loading a model\n",
    "regressor = Sequential()\n",
    "\n",
    "dropout = 0.1\n",
    "\n",
    "#regressor.add(tf.keras.layers.Masking(mask_value=-9999,input_shape= x_train[0].shape)) \n",
    "\n",
    "regressor.add(tf.keras.layers.Conv2D(filters = d, kernel_size = (3,3), padding = \"same\", activation = \"tanh\", input_shape = x_train[0].shape))\n",
    "\n",
    "\n",
    "regressor.add(tf.keras.layers.MaxPool2D(pool_size = (2,2)))\n",
    "\n",
    "\n",
    "regressor.add(tf.keras.layers.Conv2D(filters = 2*d, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
    "\n",
    "regressor.add(tf.keras.layers.MaxPool2D(pool_size = (2,2)))\n",
    "\n",
    "\n",
    "regressor.add(tf.keras.layers.Flatten())\n",
    "\n",
    "regressor.add(tf.keras.layers.Dense(units = 4*d, activation = 'relu'))\n",
    "\n",
    "regressor.add(tf.keras.layers.Dropout(0.1))\n",
    "\n",
    "regressor.add(tf.keras.layers.Dense(units = 8*d, activation = 'relu'))\n",
    "\n",
    "regressor.add(tf.keras.layers.Dropout(0.1))\n",
    "\n",
    "regressor.add(tf.keras.layers.Dense(units = 16*d, activation = 'relu'))\n",
    "\n",
    "regressor.add(tf.keras.layers.Dropout(0.1))\n",
    "\n",
    "regressor.add(tf.keras.layers.Dense(units = 32*d, activation = 'relu'))\n",
    "\n",
    "regressor.add(tf.keras.layers.Dropout(0.1))\n",
    "\n",
    "regressor.add(tf.keras.layers.Dense(units = d*d, activation = 'relu'))\n",
    "\n",
    "'''\n",
    "regressor.add(tf.keras.layers.Bidirectional(LSTM(units=n_past, return_sequences=True, input_shape = x_train[0].shape)))\n",
    "regressor.add(Dropout(dropout))\n",
    "\n",
    "regressor.add(LSTM(units = n_past-1, return_sequences = True))\n",
    "regressor.add(Dropout(dropout))\n",
    "\n",
    "regressor.add(LSTM(units = n_past-2, return_sequences = False))\n",
    "regressor.add(Dropout(dropout))\n",
    "\n",
    "regressor.add(LSTM(units = n_past, return_sequences = True))\n",
    "regressor.add(Dropout(dropout))\n",
    "\n",
    "\n",
    "regressor.add(Dense(units = n_future, activation = 'linear'))\n",
    "'''\n",
    "\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics = ['acc'])\n",
    "\n",
    "regressor.fit(x_train, y_train, validation_split=0.2, epochs = 1, batch_size = 10)\n",
    "\n",
    "model_name = '2018-num-of-fires'## CHANGE THIS (e.g. 'my_model') ##\n",
    "regressor.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, y_train.shape\n",
    "np.where(y_train > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ex = np.ones((2,4096))\n",
    "for i in range(2):   \n",
    "    ytr = y_train[i + 6]\n",
    "    for j, y in enumerate(ytr):\n",
    "        if y:\n",
    "            continue\n",
    "        else:\n",
    "            y_ex[i][j] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to run this if you are loading a model\n",
    "regressor = Sequential()\n",
    "\n",
    "regressor.add(tf.keras.layers.Conv2D(filters = d, kernel_size = (3,3), padding = \"same\", activation = \"tanh\", input_shape = x_train[0].shape))\n",
    "\n",
    "regressor.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "regressor.add(tf.keras.layers.MaxPool2D(pool_size = (2,2)))\n",
    "\n",
    "regressor.add(tf.keras.layers.Flatten())\n",
    "\n",
    "regressor.add(tf.keras.layers.Dense(units = d, activation = 'relu'))\n",
    "\n",
    "regressor.add(tf.keras.layers.Dense(units = d*d, activation = 'relu'))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "\n",
    "regressor.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['acc'])\n",
    "'''\n",
    "my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=2),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "]\n",
    "'''\n",
    "#regressor.fit(x_train[6].reshape((1, d, d, 4)), y_train[6].reshape((1,d*d)), epochs = 10, batch_size = 10)\n",
    "regressor.fit(x_train[6:8].reshape(2, d, d, 4), y_train[6:8].reshape((2, d*d)), epochs = 1, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14432, 1, 64, 64, 4), (14432, 4096))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train = x_train.reshape((int(x_train.shape[0]), 1, d, d, 4)), y_train.reshape((int(y_train.shape[0]), d*d))\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2/2 [==============================] - 14s 903ms/step - loss: 2.1707 - acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 1s 653ms/step - loss: 3.0381 - acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 2s 703ms/step - loss: 3.0399 - acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 1s 699ms/step - loss: 3.0418 - acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 2s 826ms/step - loss: 3.0418 - acc: 0.0000e+00\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 1s 641ms/step - loss: 3.0418 - acc: 0.0000e+00\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 1s 753ms/step - loss: 3.0418 - acc: 0.0000e+00\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 2s 834ms/step - loss: 3.0418 - acc: 0.0000e+00\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 1s 680ms/step - loss: 3.0418 - acc: 0.0000e+00\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 1s 675ms/step - loss: 3.0418 - acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x258e2d3cc88>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No need to run this if you are loading a model\n",
    "regressor = Sequential()\n",
    "\n",
    "regressor.add(tf.keras.layers.Conv3D(filters = d, kernel_size = (3,3, 3), padding = \"same\", activation = \"sigmoid\", input_shape = x_train[0].shape))\n",
    "\n",
    "regressor.add(tf.keras.layers.MaxPooling3D(pool_size=(2, 2, 2), padding='same'))\n",
    "\n",
    "regressor.add(tf.keras.layers.BatchNormalization(center=True, scale=True))\n",
    "\n",
    "regressor.add(Dropout(0.5))\n",
    "\n",
    "regressor.add(tf.keras.layers.Conv3D(filters = d, kernel_size = (3,3, 3), padding = \"same\", activation = \"sigmoid\"))\n",
    "\n",
    "regressor.add(tf.keras.layers.MaxPooling3D(pool_size=(2, 2, 2), padding='same'))\n",
    "\n",
    "regressor.add(tf.keras.layers.BatchNormalization(center=True, scale=True))\n",
    "\n",
    "regressor.add(Dropout(0.5))\n",
    "\n",
    "regressor.add(tf.keras.layers.Flatten())\n",
    "\n",
    "regressor.add(tf.keras.layers.Dense(units = d, activation = 'relu'))\n",
    "\n",
    "#regressor.add(tf.keras.layers.Dense(units = d, activation = 'relu'))\n",
    "\n",
    "#regressor.add(tf.keras.layers.Dense(units = 2*d*d, activation = 'relu'))\n",
    "\n",
    "#regressor.add(tf.keras.layers.Dense(units = d*d, activation = 'sigmoid'))\n",
    "regressor.add(tf.keras.layers.Dense(units = d*d, activation = 'relu'))\n",
    "\n",
    "regressor.add(tf.keras.layers.Dense(units = d*d, activation = 'relu'))\n",
    "\n",
    "regressor.add(tf.keras.layers.Dense(units = d*d, activation = 'relu'))\n",
    "\n",
    "regressor.add(tf.keras.layers.Dense(units = d*d, activation = 'relu'))\n",
    "\n",
    "regressor.add(tf.keras.layers.Dense(units = d*d, activation = 'relu'))\n",
    "\n",
    "regressor.add(tf.keras.layers.Dense(units = d*d, activation = 'relu'))\n",
    "\n",
    "regressor.add(tf.keras.layers.Dense(units = d*d, activation = 'relu'))\n",
    "\n",
    "regressor.add(tf.keras.layers.Dense(units = d*d, activation = 'relu'))\n",
    "\n",
    "regressor.add(tf.keras.layers.Dense(units = d*d, activation = 'relu'))\n",
    "\n",
    "regressor.add(tf.keras.layers.Dense(units = d*d, activation = 'relu'))\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.25)\n",
    "\n",
    "regressor.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['acc'])\n",
    "'''\n",
    "my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=2),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "]\n",
    "'''\n",
    "#regressor.fit(x_train[6].reshape((1, d, d, 4)), y_train[6].reshape((1,d*d)), epochs = 10, batch_size = 10)\n",
    "regressor.fit(x_train[37:39].reshape(2, 1, d, d, 4), y_train[37:39].reshape((2, d*d)), epochs = 10, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x258b2b54888>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAECCAYAAAD+eGJTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAASNElEQVR4nO3dfYxddZ3H8fen02lLa7EMD7XLVAY2LGoiT1uBWmIQRFnWCAQlGELKpqExugaCQcpu9sHsJivZRGs2u5BqlcawAgvWNohgM0I2CBSKoALVLUJLh5aOgjzHtjPz3T/u6dxzL/dO78zcc+5Mf59X0tzzdO/5tvfy4Xd+v/OgiMDM0jWj0wWYWWc5BMwS5xAwS5xDwCxxDgGzxDkEzBJXaghIukDSbyU9J2lVyfv+rqRBSU/nlvVI2iRpW/Z6REm1LJb0gKStkp6RdE2n6pE0R9Jjkn6Z1fK1bPnxkjZntdwhaVbRtWT77ZL0pKR7OlzHdkm/lvSUpC3Zsk79XhZIukvSb7LfzNJ21lJaCEjqAv4T+CvgQ8DnJX2orP0DtwIX1C1bBfRHxIlAfzZfhiHgKxHxQeAs4EvZv0Un6tkLnBsRpwCnAhdIOgu4CfhmVssfgRUl1AJwDbA1N9+pOgA+HhGnRsSSbL5Tv5dvAfdFxAeAU6j8+7Svlogo5Q+wFLg/N38jcGNZ+8/22Qc8nZv/LbAom14E/LbMenJ1bADO73Q9wFzgF8CZwB+AmY2+uwL335v9oM8F7gHUiTqyfW0HjqpbVvr3AxwOvACoqFrKPBw4FtiZmx/IlnXSwojYDZC9HlN2AZL6gNOAzZ2qJ2uCPwUMApuA3wGvRcRQtklZ39Vq4KvASDZ/ZIfqAAjgp5KekLQyW9aJ7+cE4PfA97LDpO9ImtfOWsoMATVYlvQ5y5LeA9wNXBsRb3SqjogYjohTqfyf+Azgg402K7IGSZ8GBiPiifzisuvIWRYRp1M5fP2SpI+VtN96M4HTgZsj4jTgbdp8GFJmCAwAi3PzvcCuEvffyB5JiwCy18Gydiypm0oA3BYRP+x0PQAR8RrwIJV+igWSZmaryviulgGfkbQduJ3KIcHqDtQBQETsyl4HgfVUwrET388AMBARm7P5u6iEQttqKTMEHgdOzHp7ZwGXAxtL3H8jG4Hl2fRyKsfmhZMkYC2wNSK+0cl6JB0taUE2fRjwCSodTw8Any2rloi4MSJ6I6KPym/jZxFxRdl1AEiaJ2n+gWngk8DTdOD7iYiXgZ2STsoWnQc829ZayuhkyXVmXAj8H5Vjzr8ved8/AHYD+6mk6woqx5z9wLbstaekWs6m0qz9FfBU9ufCTtQDnAw8mdXyNPCP2fITgMeA54D/AWaX+F2dA9zTqTqyff4y+/PMgd9qB38vpwJbsu/oR8AR7axF2U7MLFE+Y9AscQ4Bs8Q5BMwS5xAwS5xDwCxxHQmB3GmYHTVV6gDX0oxraaydtUwqBCZxafBU+cecKnWAa2nGtTTW+RCYApcGm1kbTPhkIUlLgX+OiE9l8zcCRMS/NXvPLM2OOcxjP3vpZvaE9ttOU6UOcC3NuJbGxlvLn3ibfbG30QVZzGy0sEWNLg0+c6w3zGEeZ+q8SezSzCZic/Q3XTeZEGjpMs+sA2MlwBzmTmJ3ZlaEyXQMtnRpcESsiYglEbFkqjSlzKxqMiEwFS8NNrNxmvDhQEQMSfpb4H6gC/huRDzTtsrMrBST6RMgIu4F7m1TLWbWAT5t2CxxDgGzxDkEzBLnEDBLnEPALHEOAbPEOQTMEucQMEucQ8AscQ4Bs8Q5BMwS5xAwS5xDwCxxDgGzxDkEzBLnEDBLnEPALHEOAbPEOQTMEucQMEucQ8AscQ4Bs8Q5BMwS5xAwS5xDwCxxDgGzxB00BCR9V9KgpKdzy3okbZK0LXs9otgyzaworbQEbgUuqFu2CuiPiBOB/mzezKahg4ZARPwv8Grd4ouAddn0OuDiNtdlZiWZaJ/AwojYDZC9HtO+ksysTJN6NHkrJK0EVgLMYW7RuzOzcZpoS2CPpEUA2etgsw0jYk1ELImIJd3MnuDuzKwoEw2BjcDybHo5sKE95ZhZ2VoZIvwB8AhwkqQBSSuArwPnS9oGnJ/Nm9k0dNA+gYj4fJNV57W5FjPrAJ8xaJY4h4BZ4hwCZolzCJglziFgljiHgFniHAJmiXMImCXOIWCWuMKvIsz785Pf4u6fPArApb1n1ay7fefDo9OXL/5omWWZJc0tAbPEOQTMEucQMEtcqX0Cv/vVe0b7AlZvf7hm3TDR0mfk+w4AZqv6V7ik94xJVmiWHrcEzBLnEDBLXKmHA8d/+E1uvfchAK56/9lNt7v1xYdq5ruk0enLFy+rWXfbzp+3sUKz9LglYJY4h4BZ4hwCZokrtU8Aqeb4vpmx+gvuHni0Zv7S3mVNtjSzVrglYJY4h4BZ4ko9HOhCvHfGrIbr8mcQztJIzbovHlc9PKi/+vD7uSHCKxf70MBsvNwSMEucQ8Asca08i3CxpAckbZX0jKRrsuU9kjZJ2pa9HlF8uWbWbq30CQwBX4mIX0iaDzwhaRNwFdAfEV+XtApYBdww1gc996t5fObYjzRcd3RX9SrCuWrcb9DI2yOtXX1oZo0dtCUQEbsj4hfZ9JvAVuBY4CJgXbbZOuDiooo0s+KMq09AUh9wGrAZWBgRu6ESFMAx7S7OzIrX8hChpPcAdwPXRsQbauHMv+x9K4GVAHOYO7r8XVcK5vLondhfs+6WHdVt58+o329rdZhZYy21BCR1UwmA2yLih9niPZIWZesXAYON3hsRayJiSUQs6WZ2O2o2szZqZXRAwFpga0R8I7dqI7A8m14ObGh/eWZWtFYOB5YBVwK/lvRUtuzvgK8Dd0paAbwIfK6YEs2sSIoob4jtwyd3x49+fBQAPV1dNeteHxkenX55uPaw4X1de0enrx7jCkMza2xz9PNGvNqwA81nDJolziFglrhybyqSc1nv0pr59QOPjU5f/f7GZxU24mcYmk2OWwJmiXMImCXOIWCWuFL7BIYRr490N1yXf47gv2+vvZno9X1n1W8+yv0AZpPjloBZ4hwCZokr+UajwfwZ+xuuu3PgkdHp+uHDvG/XXX3oMwjNJsctAbPEOQTMEucQMEtc6acNN0udd3JXEdbL9wO4D8CsvdwSMEucQ8AscaUeDuz49Xy+cFzj5vxYjyN3UpkVx/99mSXOIWCWOIeAWeJKvdHo6afMjp/ftwjgXc8kXJsbBlzhYUCztvKNRs2sKYeAWeJKHSLMP5p8bd3VgAu7Dmv6Pt9M1Kw4bgmYJc4hYJa4Vh5IOkfSY5J+KekZSV/Llh8vabOkbZLukDSr+HLNrN1a6RPYC5wbEW9ljyh/SNJPgOuAb0bE7ZJuAVYAN4/1QSec/Ba331s5vr98ce0w4E0vbB6dvnug9kajz+5vfHNSM5u8g7YEouKtbLY7+xPAucBd2fJ1wMWFVGhmhWqpT0BSV/ZY8kFgE/A74LWIGMo2GQCObfLelZK2SNryyisj7ajZzNqopSHCiBgGTpW0AFgPfLDRZk3euwZYA3C4euLAEN/q7Q/XbHdtX2tDfxtferxmvv7MQzMbn3GNDkTEa8CDwFnAAkkHQqQX2NXe0sysDK2MDhydtQCQdBjwCWAr8ADw2Wyz5cCGooo0s+K0cjiwCFgnqYtKaNwZEfdIeha4XdK/Ak8Cawus08wKUupVhB8+uTt+9OOjAOjp6qpZt2e42mnY3bh7AaDpnYnMrDlfRWhmTTkEzBI3ZW40amad4ZaAWeIcAmaJcwiYJc4hYJY4h4BZ4hwCZolzCJglziFgljiHgFniHAJmiXMImCXOIWCWOIeAWeIcAmaJcwiYJc4hYJY4h4BZ4hwCZolzCJglziFgljiHgFniHAJmiWs5BLLHkz8p6Z5s/nhJmyVtk3SHpFnFlWlmRRlPS+AaKg8iPeAm4JsRcSLwR2BFOwszs3K0FAKSeoG/Br6TzQs4F7gr22QdcHERBZpZsVptCawGvgoceGrokcBrETGUzQ8Ax7a5NjMrwUFDQNKngcGIeCK/uMGmDR8lLGmlpC2Stuxn7wTLNLOitPIswmXAZyRdCMwBDqfSMlggaWbWGugFdjV6c0SsAdYAHK6e8p6DbmYtOWhLICJujIjeiOgDLgd+FhFXAA8An802Ww5sKKxKMyvMZM4TuAG4TtJzVPoI1ranJDMr07geTR4RDwIPZtPPA2e0vyQzK5PPGDRLnEPALHEOAbPEjatPYDpbP/BYzfwfRvaNTl/9/rNr1n3t+eopEf90wl82/czbdz5cM3/54o9OpkSzjnBLwCxxDgGzxDkEzBKXTJ/Ac0MjNfN/1lW9/GGsY/t/eeHxmnX/cPxHGm5nNl25JWCWOIeAWeKSORzomTFUM3/F4uqw4Pd3/rzp+/ZHV838rS8+NDp9Vd3Qotl05JaAWeIcAmaJcwiYJU4R5d3s53D1xJk6r7DPv+mFzTXzf9FdHQZ8PXeaMMAcNc+/l4er07NUO7T4xePcD2DTz+bo5414tdFtAd0SMEudQ8AscYfUEOENx59ZM58f+qsfzrt74NHR6Ut7z6pZt/Gl6lmCw2McLt058EjN/GW9S1sv1myKcEvALHEOAbPEOQTMEndI9Qnkj/MBLu1d1nTbfD/A6u21VxHuGa4OC9bfdeiWHdXThkfCGWrTn3/FZolzCJgl7pA6Y7Be/vBguO55qa0O59UPA07kM8w6zWcMmllTLXUMStoOvAkMA0MRsURSD3AH0AdsBy6LiD8WU6aZFWU8LYGPR8SpEbEkm18F9EfEiUB/Nm9m08xkhggvAs7JptdReVDpDZOsp62ez91M6Pq+2uP3Vu8QNFB7QyKuy33Ou4cka08/NpsOWm0JBPBTSU9IWpktWxgRuwGy12OKKNDMitVqS2BZROySdAywSdJvWt1BFhorAeYwdwIlmlmRWgqBiNiVvQ5KWg+cAeyRtCgidktaBAw2ee8aYA1UhgjbU3ZV/pkBc2d016zbMTRcv/mo986Y1dLnX9fXfBjwzZGhpuvMpouDHg5Imidp/oFp4JPA08BGYHm22XJgQ1FFmllxWmkJLATWSzqw/X9HxH2SHgfulLQCeBH4XHFlmllRDhoCEfE8cEqD5a8A5Z3+Z2aFOKRPG/6v3BV/vkGopcynDZtZUw4Bs8QdUjcVGcs9Lz1RM//6yJ9Gp3cN1z5v8H1d1aHFV4ZrW1BHdlUPn7qpXedHldt05JaAWeIcAmaJcwiYJW7a9wmsH3hsdHo/tacJX9Z7dsPtAPJPGLy+r/bqv/zQ4rV9zW9Wmn+4idl05ZaAWeIcAmaJm/aHAzuGqlfyXdtXO0SXv0noqyO1hworxriRyNyG51VV/MeO6iFAq1cimk1lbgmYJc4hYJY4h4BZ4qZ9n0C+HyB/vA5wWe5ZhGtzNxatV/++sdZ9+bjmQ4Zm05FbAmaJcwiYJW7a31Qkf9bevrq/S/6swJ4ZtUc+70R1yPDKxbVN/PzNS2eodrxwb4zQTP3nmE0VvqmImTXlEDBLnEPALHHTfogwn2IL6o77888GvH/XUzXrdu57p+ln5u8QlD/1GHzcb4cetwTMEucQMEvctB8izKs/K3D+jK4mW0I31XV7hvfVva86knJFXfP/ltwNR77gZxnYNOEhQjNrqqUQkLRA0l2SfiNpq6SlknokbZK0LXs9ouhizaz9Wm0JfAu4LyI+QOW5hFuBVUB/RJwI9GfzZjbNHHSIUNLhwMeAqwAiYh+wT9JFwDnZZuuAB4EbiihyLPl+gHmqzbTLepc2fV/+xqNjHdvXDxHmb15qdihopSVwAvB74HuSnpT0HUnzgIURsRsgez2mwDrNrCCthMBM4HTg5og4DXibcTT9Ja2UtEXSlv3snWCZZlaUVs4YHAAGImJzNn8XlRDYI2lRROyWtAgYbPTmiFgDrIHKEGEbaq4x1g1D8+qHDy8Z45kE78T+0ek3R8obQjXrhIO2BCLiZWCnpJOyRecBzwIbgeXZsuXAhkIqNLNCtXrtwJeB2yTNAp4H/oZKgNwpaQXwIvC5Yko0syK1FAIR8RSwpMGq4k7/M7NSTPurCPNDeGMNCdb3HazeXr170CW9tQ8t+cb26mde19f8M79d189wdYv9E2ZTiU8bNkucQ8AscdP+cGDX0MSG8I6b2fyvnj8EePcZg9V1bv7bocAtAbPEOQTMEucQMEtcqXcWkvR7YAdwFPCH0nbc3FSpA1xLM66lsfHWclxEHN1oRakhMLpTaUtENDr5KMk6wLU041oaa2ctPhwwS5xDwCxxnQqBNR3ab72pUge4lmZcS2Ntq6UjfQJmNnX4cMAscQ4Bs8Q5BMwS5xAwS5xDwCxx/w8CH/CzqEbA2wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAECCAYAAAD+eGJTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAANfUlEQVR4nO3df+hd9X3H8edrJiY1rcS0GqKRpUKw+seM8sUfOEpranWuVP/Q0VJGNgL5xw3LCm3cYKwwWP2n2j9GIaht/nCtztYlSNGGVBmDERtrbNXoYp3TkNSv3ZR2wqKx7/1xj9t34X73vfnmnnsTP88HfDn3fO65fF587+WVc8733JxUFZLa9VvTDiBpuiwBqXGWgNQ4S0BqnCUgNc4SkBo30RJIcn2SF5K8mGTrhOe+N8lskmfmjK1KsivJgW551oSynJ/ksST7kzyb5LZp5UmyPMkTSZ7usny1G/9okj1dlvuTnN53lm7e05I8leThKed4OcnPkuxLsrcbm9bnZWWSB5M8331mrhpnlomVQJLTgL8Ffg+4GPh8kosnNT/wbeD6Y8a2Aruraj2wu1ufhKPAl6rqIuBK4NbudzGNPEeAa6rqEmADcH2SK4E7gDu7LG8AmyeQBeA2YP+c9WnlAPhkVW2oqplufVqfl28Aj1TVx4BLGPx+xpelqibyA1wFPDpn/Xbg9knN3825DnhmzvoLwJru8RrghUnmmZNjB3DttPMAZwA/Aa4AfgksGfbe9Tj/2u4DfQ3wMJBp5Ojmehn4yDFjE39/gDOBfwXSV5ZJHg6cB7w6Z/1gNzZNq6vqMEC3PGfSAZKsAy4F9kwrT7cLvg+YBXYBPwferKqj3SaTeq/uAr4M/KZb//CUcgAU8MMkTybZ0o1N4/25AHgd+FZ3mHR3khXjzDLJEsiQsaavWU7yQeB7wBer6lfTylFV71bVBgb/El8OXDRssz4zJPkMMFtVT84dnnSOOa6uqssYHL7emuTjE5r3WEuAy4BvVtWlwFuM+TBkkiVwEDh/zvpa4NAE5x/mtSRrALrl7KQmTrKUQQHcV1Xfn3YegKp6E3icwXmKlUmWdE9N4r26GvhskpeB7zI4JLhrCjkAqKpD3XIWeIhBOU7j/TkIHKyqPd36gwxKYWxZJlkCPwbWd2d7Twc+B+yc4PzD7AQ2dY83MTg2712SAPcA+6vq69PMk+TsJCu7xx8APsXgxNNjwM2TylJVt1fV2qpax+Cz8aOq+sKkcwAkWZHkQ+89Bj4NPMMU3p+q+gXwapILu6GNwHNjzTKJkyxzTmbcAPwLg2POv5jw3N8BDgPvMGjXzQyOOXcDB7rlqgll+V0Gu7U/BfZ1PzdMIw/wO8BTXZZngL/sxi8AngBeBP4eWDbB9+oTwMPTytHN+XT38+x7n9Upfl42AHu79+gfgLPGmSXdJJIa5RWDUuMsAalxloDUOEtAapwlIDVuKiUw5zLMqTpZcoBZ5mOW4caZ5YRK4AS+Gnyy/DJPlhxglvmYZbjpl8BJ8NVgSWOw6IuFklwF/FVVXdet3w5QVX8z32tOz7Jazgre4QhLWbaoecfpZMkBZpmPWYY73iz/xVu8XUeGfSGLJcMGRzTsq8FX/H8vWM4KrsjGE5hS0mLsqd3zPnciJTDS1zy7ExhbAJZzxglMJ6kPJ3JicKSvBlfVtqqaqaqZk2VXStL/OpESOBm/GizpOC36cKCqjib5E+BR4DTg3qp6dmzJJE3EiZwToKp+APxgTFkkTYGXDUuNswSkxlkCUuMsAalxloDUOEtAapwlIDXOEpAaZwlIjbMEpMZZAlLjLAGpcZaA1DhLQGqcJSA1zhKQGmcJSI2zBKTGWQJS4ywBqXGWgNQ4S0BqnCUgNc4SkBpnCUiNswSkxi1YAknuTTKb5Jk5Y6uS7EpyoFue1W9MSX0ZZU/g28D1x4xtBXZX1Xpgd7cu6RS0YAlU1T8C/3HM8I3A9u7xduCmMeeSNCGLPSewuqoOA3TLc8YXSdIkndCtyUeRZAuwBWA5Z/Q9naTjtNg9gdeSrAHolrPzbVhV26pqpqpmlrJskdNJ6stiS2AnsKl7vAnYMZ44kiZtlD8Rfgf4Z+DCJAeTbAa+Blyb5ABwbbcu6RS04DmBqvr8PE9tHHMWSVPgFYNS4ywBqXGWgNQ4S0BqnCUgNc4SkBpnCUiNswSkxlkCUuMsAalxloDUOEtAapwlIDXOEpAaZwlIjbMEpMZZAlLjLAGpcZaA1DhLQGqcJSA1zhKQGmcJSI2zBKTGWQJS4ywBqXGj3Ivw/CSPJdmf5Nkkt3Xjq5LsSnKgW57Vf1xJ4zbKnsBR4EtVdRFwJXBrkouBrcDuqloP7O7WJZ1iFiyBqjpcVT/pHv8a2A+cB9wIbO822w7c1FdISf05rnMCSdYBlwJ7gNVVdRgGRQGcM+5wkvo3cgkk+SDwPeCLVfWr43jdliR7k+x9hyOLySipRyOVQJKlDArgvqr6fjf8WpI13fNrgNlhr62qbVU1U1UzS1k2jsySxmiUvw4EuAfYX1Vfn/PUTmBT93gTsGP88ST1bckI21wN/CHwsyT7urE/B74GPJBkM/AKcEs/ESX1acESqKp/AjLP0xvHG0fSpHnFoNQ4S0BqnCUgNc4SkBpnCUiNswSkxlkCUuMsAalxloDUuFEuG5ZOOo8e2rfwRj257twNU5u7D+4JSI2zBKTGWQJS4ywBqXGWgNQ4S0BqnCUgNc4SkBpnCUiNswSkxlkCUuMsAalxloDUOEtAapwlIDXOEpAaN8oNSZcneSLJ00meTfLVbvyjSfYkOZDk/iSn9x9X0riNsidwBLimqi4BNgDXJ7kSuAO4s6rWA28Am/uLKakvC5ZADfxnt7q0+yngGuDBbnw7cFMvCSX1aqRzAklO625LPgvsAn4OvFlVR7tNDgLnzfPaLUn2Jtn7DkfGkVnSGI1UAlX1blVtANYClwMXDdtsntduq6qZqppZyrLFJ5XUi+P660BVvQk8DlwJrEzy3v9WvBY4NN5okiZhlL8OnJ1kZff4A8CngP3AY8DN3WabgB19hZTUn1HuO7AG2J7kNAal8UBVPZzkOeC7Sf4aeAq4p8ecknqyYAlU1U+BS4eMv8Tg/ICkU5hXDEqNswSkxnkvQp0Spnnvwfc79wSkxlkCUuMsAalxloDUOEtAapwlIDXOEpAaZwlIjbMEpMZZAlLjLAGpcZaA1DhLQGqcJSA1zhKQGmcJSI2zBKTGWQJS4ywBqXGWgNQ4S0BqnCUgNW7kEuhuT/5Ukoe79Y8m2ZPkQJL7k5zeX0xJfTmePYHbGNyI9D13AHdW1XrgDWDzOINJmoyRSiDJWuD3gbu79QDXAA92m2wHbuojoKR+jboncBfwZeA33fqHgTer6mi3fhA4b8zZJE3AgiWQ5DPAbFU9OXd4yKY1z+u3JNmbZO87HFlkTEl9GeVehFcDn01yA7AcOJPBnsHKJEu6vYG1wKFhL66qbcA2gDOzamhRSJqeBfcEqur2qlpbVeuAzwE/qqovAI8BN3ebbQJ29JZSUm9O5DqBrwB/luRFBucI7hlPJEmTdFy3Jq+qx4HHu8cvAZePP5KkSTquEpCm5bpzN/yf9UcP7ZtSkvcfLxuWGmcJSI3zcECnpGMPD7R47glIjbMEpMZZAlLjLAGpcZaA1DhLQGqcJSA1zhKQGmcJSI2zBKTGWQJS4ywBqXGWgNQ4S0BqnCUgNc4SkBpnCUiNswSkxlkCUuMsAalxloDUOEtAatxI/+V4kpeBXwPvAkeraibJKuB+YB3wMvAHVfVGPzEl9eV49gQ+WVUbqmqmW98K7K6q9cDubl3SKeZEDgduBLZ3j7cDN514HEmTNmoJFPDDJE8m2dKNra6qwwDd8pw+Akrq16i3Ibu6qg4lOQfYleT5USfoSmMLwHLOWERESX0aaU+gqg51y1ngIeBy4LUkawC65ew8r91WVTNVNbOUZeNJLWlsFiyBJCuSfOi9x8CngWeAncCmbrNNwI6+QkrqzyiHA6uBh5K8t/3fVdUjSX4MPJBkM/AKcEt/MSX1ZcESqKqXgEuGjP87sLGPUJImZ9QTgxri0UP7pjr/dedumOr8en/wsmGpcZaA1DhLQGqcJSA1zhKQGmcJSI2zBKTGWQJS4ywBqXGWgNQ4S0BqnCUgNc4SkBpnCUiNswSkxlkCUuMsAalxloDUOEtAapwlIDXOEpAaZwlIjbMEpMZZAlLjLAGpcSOVQJKVSR5M8nyS/UmuSrIqya4kB7rlWX2HlTR+o+4JfAN4pKo+xuC+hPuBrcDuqloP7O7WJZ1iFrwXYZIzgY8DfwRQVW8Dbye5EfhEt9l24HHgK32EnGva9/+T3m9G2RO4AHgd+FaSp5LcnWQFsLqqDgN0y3N6zCmpJ6OUwBLgMuCbVXUp8BbHseufZEuSvUn2vsORRcaU1JdRSuAgcLCq9nTrDzIohdeSrAHolrPDXlxV26pqpqpmlrJsHJkljdGCJVBVvwBeTXJhN7QReA7YCWzqxjYBO3pJKKlXC54Y7PwpcF+S04GXgD9mUCAPJNkMvALc0k9ESX0aqQSqah8wM+SpjeONI2nSvGJQapwlIDXOEpAaZwlIjbMEpMZZAlLjLAGpcamqyU2WvA78G/AR4JcTm3h+J0sOMMt8zDLc8Wb57ao6e9gTEy2B/5k02VtVwy4+ajIHmGU+ZhlunFk8HJAaZwlIjZtWCWyb0rzHOllygFnmY5bhxpZlKucEJJ08PByQGmcJSI2zBKTGWQJS4ywBqXH/DRYCXmkFTSQzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_test = regressor.predict(x_train[37].reshape((1, 1, d, d, 4)))\n",
    "b = predict_test\n",
    "plt.matshow(b.reshape((64,64)))\n",
    "predict_test = regressor.predict(x_train[38].reshape((1, 1, d, d, 4)))\n",
    "p = predict_test\n",
    "plt.matshow(p.reshape((64,64)))\n",
    "plt.matshow(y_train[38].reshape((64,64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
       "       25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
       "       25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 31, 31, 31, 31, 31, 31, 31,\n",
       "       31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n",
       "       31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n",
       "       31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n",
       "       31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n",
       "       31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n",
       "       31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n",
       "       31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n",
       "       31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n",
       "       31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n",
       "       31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n",
       "       31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 36, 36, 36, 36, 36, 36,\n",
       "       36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36,\n",
       "       36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36,\n",
       "       36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 38, 38, 38, 38, 38, 38, 38,\n",
       "       38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
       "       38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
       "       38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
       "       38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
       "       38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
       "       38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
       "       38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
       "       38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
       "       38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
       "       38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
       "       38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
       "       38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
       "       38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
       "       38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
       "       38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
       "       38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
       "       38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
       "       38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
       "       38, 38, 38, 38, 38, 38, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41,\n",
       "       41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41,\n",
       "       41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41,\n",
       "       41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41,\n",
       "       41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(y_train>0)[0][1000:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "This follows the general layout of https://keras.io/examples/vision/conv_lstm/. I haven't had much success with it\n",
    "    \n",
    "<b>Train your RNN:</b> Train your RNN using the training data.\n",
    "    \n",
    "    You might notice a lot of arbitrary values here (these are things we will want to change and test)\n",
    "    - Dropout: High dropout leads to more generalization. Low dropout takes advantage of more data but overfits more easily\n",
    "    - # of hidden layers: Right now, there are 2 hidden layers (+ 1 at the end). We can change this\n",
    "    - # units at hidden layers: Right now, units decrease by 1 at each hidden layer. This is arbitrary and can be changed\n",
    "    - optimizer --> 'adam' works but I don't know what it does\n",
    "    - epochs --> We can raise this above 1 but I don't notice that changing much when I have done so\n",
    "    - batch_size --> Higher batch size leads to more generalization\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = x_train.reshape((int(408 * 256**2/d**2), 4, d, d, 1)), y_train.reshape((int(408 * 256**2/d**2), d, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to run this if you are loading a model\n",
    "regressor = Sequential()\n",
    "\n",
    "dropout = 0.1\n",
    "# Construct the input layer with no definite frame size.\n",
    "inp = tf.keras.layers.Input(shape=(None, *x_train.shape[2:]))\n",
    "#regressor.add(tf.keras.layers.Masking(mask_value=-9999,input_shape= x_train[0].shape)) \n",
    "x = tf.keras.layers.ConvLSTM2D(filters = 64, kernel_size = (5,5), padding=\"same\", return_sequences = True, activation = \"relu\")(inp)\n",
    "\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "x = tf.keras.layers.ConvLSTM2D(\n",
    "    filters=64,\n",
    "    kernel_size=(3, 3),\n",
    "    padding=\"same\",\n",
    "    return_sequences=True,\n",
    "    activation=\"relu\",\n",
    ")(x)\n",
    "\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.ConvLSTM2D(\n",
    "    filters=64,\n",
    "    kernel_size=(1, 1),\n",
    "    padding=\"same\",\n",
    "    return_sequences=True,\n",
    "    activation=\"relu\",\n",
    ")(x)\n",
    "x = tf.keras.layers.ConvLSTM2D(\n",
    "    filters=1,\n",
    "    kernel_size=(1, 1),\n",
    "    padding=\"same\",\n",
    "    return_sequences=True,\n",
    "    activation=\"relu\",\n",
    ")(x)\n",
    "\n",
    "'''\n",
    "x = tf.keras.layers.Conv3D(\n",
    "    filters=1, kernel_size=(3, 3, 3), activation=\"sigmoid\", padding=\"same\"\n",
    ")(x)\n",
    "'''\n",
    "\n",
    "model = tf.keras.models.Model(inp, x)\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.MeanAbsoluteError(), optimizer=tf.keras.optimizers.Adam(),\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=1,\n",
    "    epochs=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Load pre-trained model:</b> \n",
    "    If you want to load a model instead, use this\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model_name = '' ## CHANGE THIS (e.g. 'my_model') ##\n",
    "regressor = load_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Save pre-trained model:</b> \n",
    "    If you want to save a model instead, use this\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '1d-output'## CHANGE THIS (e.g. 'my_model') ##\n",
    "regressor.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Input Data for Testing:</b> \n",
    "    Below, we get the input data from March 2018! \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "path = path = \"C:\\\\Users\\\\nicoc\\\\Desktop\\\\Stanford\\\\OneDrive\\\\OneDrive - Stanford\\\\Courses\\\\CS229\\\\finalproject\\\\data\\\\United_States_Fires\\\\United_States_2018_Fires\\\\nov\\\\storage\"\n",
    "x_test, y_test  = nn_train_test(path, n_past = 3, n_comp = n_comp, n_future =1, d = d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Check:</b> I've noticed the first dimensions of these sometimes these don't match. They should.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = x_test.reshape((x_test.shape[0], d, d, 4)), y_test.reshape((int(y_test.shape[0]), d, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array([np.sum(y) for y in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test[:, :, :, 3][x_test[:, :, :, 3] == -9999] = 0\n",
    "for i in range(len(x_test)):\n",
    "    if x_test[i,:,:,3].sum() > 0:\n",
    "        x_test[i,:,:,3] = x_test[i,:,:,3] / x_test[i,:,:,3].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b> Predict Step:</b> This makes your predictions using your model and reshapes them into a list of 2D arrays\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "predict_test = regressor.predict(x_test)\n",
    "p = predict_test\n",
    "shape = predict_test.shape\n",
    "#p = p.reshape((p.shape[0], d, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 6\n",
    "p[k], y_test[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(p[3].reshape((64,64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_train = regressor.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_train, 'blue')\n",
    "plt.plot(predict_train, 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(y_test, 'blue')\n",
    "plt.plot(p, 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array([np.sum(y) for y in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test[599,:,:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(y_test[666])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(p[60])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
